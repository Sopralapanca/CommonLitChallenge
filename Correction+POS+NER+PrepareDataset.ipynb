{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sopralapanca/CommonLitChallenge/blob/main/Correction%2BPOS%2BNER%2BPrepareDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAVORO PER SARDE:\n",
        "1. Aggiungere altri todo\n",
        "2. Imparare ad essere umile\n",
        "3. Tifare il Campobasso\n",
        "\n",
        "TODO: CONTROLLARE I PARAGRAFI\n",
        "\n",
        "1. Controllare se non ci sono whitespace dopo gli .\\n Maiuscola\n",
        "2. Score value spacy per identificare la similarità, potremmo usarla per paragrafi diversi.\n",
        "\n",
        "TODO: CONTROLLARE I NOMI PROPRI\n",
        "1. Si potrebbe effettuare il lower() prima di aggiungere le parole allo spellchecker sperando che ci siano i nomi propri necessari alla stesura del riassunto all'interno del prompt\n",
        "2. Controllare i nomi propri facendo ner e non tenerli in considerazione per eventuali errori, ma confrontarli con eventuali nomi propri all'interno dei prompt (selezionare i soggetti che iniziano per le stesse lettere e poi calcolarcisi l'edit distance ==> wording, entro un certo range viene considerato match e si salva una feature per il numero di match con i prompt ==> content)\n",
        "\n"
      ],
      "metadata": {
        "id": "8IDA3TWbPR4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data importing and setup"
      ],
      "metadata": {
        "id": "muui6d4mCFm4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weCgm4nI56zm",
        "outputId": "c25992ca-ab15-45bc-a8f9-e7bdce900023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (1.12)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=6d62c50dcdc7522638a48f626af1503de30c7655752e37d0eb2b65370937b6c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: pyspellchecker, autocorrect\n",
            "Successfully installed autocorrect-2.6.1 pyspellchecker-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker sympy spacy autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-GRGqEVbG2D",
        "outputId": "5ca7b403-f9bc-4988-fb7e-b780cb792c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-02 09:18:12.466657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-02 09:18:13.937039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNgQ83Og83ke",
        "outputId": "9492d667-908a-4c2d-8c4c-724ee86fccd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CommonLitChallenge'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 235 (delta 46), reused 58 (delta 44), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (235/235), 16.33 MiB | 12.66 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/Sopralapanca/CommonLitChallenge.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3bk8Dmz-GWE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "summaries_train_path = \"/content/CommonLitChallenge/data/summaries_train.csv\"\n",
        "prompt_train_path = \"/content/CommonLitChallenge/data/prompts_train.csv\"\n",
        "\n",
        "train_data = pd.read_csv(summaries_train_path, sep=',', index_col=0)\n",
        "prompt_data = pd.read_csv(prompt_train_path, sep=',', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell Correction"
      ],
      "metadata": {
        "id": "draRz0ddB_G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the list of words"
      ],
      "metadata": {
        "id": "zkz-UMcvBvst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8XMtG9s8_c8"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "word_list = []\n",
        "for prompt, query, title in zip(prompt_data.prompt_text.tolist(), prompt_data.prompt_question.tolist(), prompt_data.prompt_title.tolist()):\n",
        "  word_list.append(prompt.replace('\\n', ' ').split())\n",
        "  word_list.append(query.replace('\\n', ' ').split())\n",
        "  word_list.append(title.replace('\\n', ' ').split())\n",
        "token_list = list(itertools.chain(*word_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correction of the summaries"
      ],
      "metadata": {
        "id": "ULONxuzDB7f_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drUzHk2u75tH"
      },
      "outputs": [],
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "class SpellCorrector():\n",
        "  def __init__(self) -> None:\n",
        "    self.speller = Speller(lang='en')\n",
        "  def initialize_tokens(self, tokens):\n",
        "    self.speller.nlp_data.update({token:1000 for token in tokens})\n",
        "  def run(self, dataframe):\n",
        "    dataframe[\"fixed_summary_text\"] = dataframe[\"text\"].apply(\n",
        "            lambda x: self.speller(x)\n",
        "        )\n",
        "    return dataframe[\"fixed_summary_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdDSaEhj9JMV"
      },
      "outputs": [],
      "source": [
        "spell_enchanter = SpellCorrector()\n",
        "spell_enchanter.initialize_tokens(token_list)\n",
        "train_data[\"corrected_text\"] = spell_enchanter.run(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text processing"
      ],
      "metadata": {
        "id": "bpl5SG5yCQ1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtNSmpUJKFs4",
        "outputId": "93287188-9422-4ef7-92a4-9b4d00f7b127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocessText(text, removal=True):\n",
        "    # replace newline with space\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace('\\r', ' ')\n",
        "\n",
        "    # Normalize spaces around punctuation marks\n",
        "    text = re.sub(r\"[^A-Za-z0-9']\", r' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Replace curly apostrophe with straight single quote\n",
        "    text = text.replace('’', \"'\")\n",
        "\n",
        "    # Normalize spaces around punctuation marks\n",
        "    text = text.strip()\n",
        "\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    if removal:\n",
        "      # split text\n",
        "      words = text.split()\n",
        "\n",
        "      # stop word removal\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "\n",
        "      # stemming\n",
        "      # words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "      # lemmatization\n",
        "      words = [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "      text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess the text\n",
        "prompt_data[\"prompt_text_preprocessed\"] =   prompt_data[\"prompt_text\"].apply(lambda x: preprocessText(x))\n",
        "train_data[\"corrected_text_preprocessed\"] = train_data[\"corrected_text\"].apply(lambda x: preprocessText(x))\n",
        "\n",
        "train_data[\"text_pre_withstop\"] =           train_data[\"text\"].apply(lambda x: preprocessText(x, False))\n",
        "train_data[\"corrected_text_pre_withstop\"] = train_data[\"corrected_text\"].apply(lambda x: preprocessText(x, False))\n",
        "# Count the length of text\n",
        "prompt_data[\"prompt_text_length\"] = prompt_data[\"prompt_text\"].apply(len)\n",
        "train_data[\"text_length\"] =         train_data[\"corrected_text\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams finding"
      ],
      "metadata": {
        "id": "O0YUWjNOCK7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAGW8zyP8BVL",
        "outputId": "e42364b5-ecee-457e-a3fe-47dd74531ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the required resources for tokenization\n",
        "\n",
        "def count_ngrams(text, n):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  ngram_counts = Counter(ngrams(words, n))\n",
        "  return ngram_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We1dhK1SBIvB"
      },
      "outputs": [],
      "source": [
        "del_columns = []\n",
        "\n",
        "for n in range(2,5):\n",
        "  col = f\"{n}grams-prompttext-count\"\n",
        "  prompt_data[col] = prompt_data.apply(lambda row: count_ngrams(row[\"prompt_text\"], n), axis=1)\n",
        "  del_columns.append(col)\n",
        "\n",
        "  col = f\"{n}grams-correctedtext-count\"\n",
        "  train_data[col] = train_data.apply(lambda row: count_ngrams(row[\"corrected_text\"], n), axis=1)\n",
        "  del_columns.append(col)\n",
        "\n",
        "  col = f\"{n}grams-text-count\"\n",
        "  train_data[col] = train_data.apply(lambda row: count_ngrams(row[\"text\"], n), axis=1)\n",
        "  del_columns.append(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS counter"
      ],
      "metadata": {
        "id": "bCdvELSBfI3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Glossary def.\n",
        "GLOSSARY = {\n",
        "    # POS tags\n",
        "    # Universal POS Tags\n",
        "    # http://universaldependencies.org/u/pos/\n",
        "    \"ADJ\": \"adjective\",\n",
        "    \"ADP\": \"adposition\",\n",
        "    \"ADV\": \"adverb\",\n",
        "    \"AUX\": \"auxiliary\",\n",
        "    \"CONJ\": \"conjunction\",\n",
        "    \"CCONJ\": \"coordinating conjunction\",\n",
        "    \"DET\": \"determiner\",\n",
        "    \"INTJ\": \"interjection\",\n",
        "    \"NOUN\": \"noun\",\n",
        "    \"NUM\": \"numeral\",\n",
        "    \"PART\": \"particle\",\n",
        "    \"PRON\": \"pronoun\",\n",
        "    \"PROPN\": \"proper noun\",\n",
        "    \"PUNCT\": \"punctuation\",\n",
        "    \"SCONJ\": \"subordinating conjunction\",\n",
        "    \"SYM\": \"symbol\",\n",
        "    \"VERB\": \"verb\",\n",
        "    \"X\": \"other\",\n",
        "    \"EOL\": \"end of line\",\n",
        "    \"SPACE\": \"space\",\n",
        "    # POS tags (English)\n",
        "    # OntoNotes 5 / Penn Treebank\n",
        "    # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "    \".\": \"punctuation mark, sentence closer\",\n",
        "    \",\": \"punctuation mark, comma\",\n",
        "    \"-LRB-\": \"left round bracket\",\n",
        "    \"-RRB-\": \"right round bracket\",\n",
        "    \"``\": \"opening quotation mark\",\n",
        "    '\"\"': \"closing quotation mark\",\n",
        "    \"''\": \"closing quotation mark\",\n",
        "    \":\": \"punctuation mark, colon or ellipsis\",\n",
        "    \"$\": \"symbol, currency\",\n",
        "    \"#\": \"symbol, number sign\",\n",
        "    \"AFX\": \"affix\",\n",
        "    \"CC\": \"conjunction, coordinating\",\n",
        "    \"CD\": \"cardinal number\",\n",
        "    \"DT\": \"determiner\",\n",
        "    \"EX\": \"existential there\",\n",
        "    \"FW\": \"foreign word\",\n",
        "    \"HYPH\": \"punctuation mark, hyphen\",\n",
        "    \"IN\": \"conjunction, subordinating or preposition\",\n",
        "    \"JJ\": \"adjective (English), other noun-modifier (Chinese)\",\n",
        "    \"JJR\": \"adjective, comparative\",\n",
        "    \"JJS\": \"adjective, superlative\",\n",
        "    \"LS\": \"list item marker\",\n",
        "    \"MD\": \"verb, modal auxiliary\",\n",
        "    \"NIL\": \"missing tag\",\n",
        "    \"NN\": \"noun, singular or mass\",\n",
        "    \"NNP\": \"noun, proper singular\",\n",
        "    \"NNPS\": \"noun, proper plural\",\n",
        "    \"NNS\": \"noun, plural\",\n",
        "    \"PDT\": \"predeterminer\",\n",
        "    \"POS\": \"possessive ending\",\n",
        "    \"PRP\": \"pronoun, personal\",\n",
        "    \"PRP$\": \"pronoun, possessive\",\n",
        "    \"RB\": \"adverb\",\n",
        "    \"RBR\": \"adverb, comparative\",\n",
        "    \"RBS\": \"adverb, superlative\",\n",
        "    \"RP\": \"adverb, particle\",\n",
        "    \"TO\": 'infinitival \"to\"',\n",
        "    \"UH\": \"interjection\",\n",
        "    \"VB\": \"verb, base form\",\n",
        "    \"VBD\": \"verb, past tense\",\n",
        "    \"VBG\": \"verb, gerund or present participle\",\n",
        "    \"VBN\": \"verb, past participle\",\n",
        "    \"VBP\": \"verb, non-3rd person singular present\",\n",
        "    \"VBZ\": \"verb, 3rd person singular present\",\n",
        "    \"WDT\": \"wh-determiner\",\n",
        "    \"WP\": \"wh-pronoun, personal\",\n",
        "    \"WP$\": \"wh-pronoun, possessive\",\n",
        "    \"WRB\": \"wh-adverb\",\n",
        "    \"SP\": \"space (English), sentence-final particle (Chinese)\",\n",
        "    \"ADD\": \"email\",\n",
        "    \"NFP\": \"superfluous punctuation\",\n",
        "    \"GW\": \"additional word in multi-word expression\",\n",
        "    \"XX\": \"unknown\",\n",
        "    \"BES\": 'auxiliary \"be\"',\n",
        "    \"HVS\": 'forms of \"have\"',\n",
        "    \"_SP\": \"whitespace\",\n",
        "    # POS Tags (German)\n",
        "    # TIGER Treebank\n",
        "    # http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf\n",
        "    \"$(\": \"other sentence-internal punctuation mark\",\n",
        "    \"$,\": \"comma\",\n",
        "    \"$.\": \"sentence-final punctuation mark\",\n",
        "    \"ADJA\": \"adjective, attributive\",\n",
        "    \"ADJD\": \"adjective, adverbial or predicative\",\n",
        "    \"APPO\": \"postposition\",\n",
        "    \"APPR\": \"preposition; circumposition left\",\n",
        "    \"APPRART\": \"preposition with article\",\n",
        "    \"APZR\": \"circumposition right\",\n",
        "    \"ART\": \"definite or indefinite article\",\n",
        "    \"CARD\": \"cardinal number\",\n",
        "    \"FM\": \"foreign language material\",\n",
        "    \"ITJ\": \"interjection\",\n",
        "    \"KOKOM\": \"comparative conjunction\",\n",
        "    \"KON\": \"coordinate conjunction\",\n",
        "    \"KOUI\": 'subordinate conjunction with \"zu\" and infinitive',\n",
        "    \"KOUS\": \"subordinate conjunction with sentence\",\n",
        "    \"NE\": \"proper noun\",\n",
        "    \"NNE\": \"proper noun\",\n",
        "    \"PAV\": \"pronominal adverb\",\n",
        "    \"PROAV\": \"pronominal adverb\",\n",
        "    \"PDAT\": \"attributive demonstrative pronoun\",\n",
        "    \"PDS\": \"substituting demonstrative pronoun\",\n",
        "    \"PIAT\": \"attributive indefinite pronoun without determiner\",\n",
        "    \"PIDAT\": \"attributive indefinite pronoun with determiner\",\n",
        "    \"PIS\": \"substituting indefinite pronoun\",\n",
        "    \"PPER\": \"non-reflexive personal pronoun\",\n",
        "    \"PPOSAT\": \"attributive possessive pronoun\",\n",
        "    \"PPOSS\": \"substituting possessive pronoun\",\n",
        "    \"PRELAT\": \"attributive relative pronoun\",\n",
        "    \"PRELS\": \"substituting relative pronoun\",\n",
        "    \"PRF\": \"reflexive personal pronoun\",\n",
        "    \"PTKA\": \"particle with adjective or adverb\",\n",
        "    \"PTKANT\": \"answer particle\",\n",
        "    \"PTKNEG\": \"negative particle\",\n",
        "    \"PTKVZ\": \"separable verbal particle\",\n",
        "    \"PTKZU\": '\"zu\" before infinitive',\n",
        "    \"PWAT\": \"attributive interrogative pronoun\",\n",
        "    \"PWAV\": \"adverbial interrogative or relative pronoun\",\n",
        "    \"PWS\": \"substituting interrogative pronoun\",\n",
        "    \"TRUNC\": \"word remnant\",\n",
        "    \"VAFIN\": \"finite verb, auxiliary\",\n",
        "    \"VAIMP\": \"imperative, auxiliary\",\n",
        "    \"VAINF\": \"infinitive, auxiliary\",\n",
        "    \"VAPP\": \"perfect participle, auxiliary\",\n",
        "    \"VMFIN\": \"finite verb, modal\",\n",
        "    \"VMINF\": \"infinitive, modal\",\n",
        "    \"VMPP\": \"perfect participle, modal\",\n",
        "    \"VVFIN\": \"finite verb, full\",\n",
        "    \"VVIMP\": \"imperative, full\",\n",
        "    \"VVINF\": \"infinitive, full\",\n",
        "    \"VVIZU\": 'infinitive with \"zu\", full',\n",
        "    \"VVPP\": \"perfect participle, full\",\n",
        "    \"XY\": \"non-word containing non-letter\",\n",
        "    # POS Tags (Chinese)\n",
        "    # OntoNotes / Chinese Penn Treebank\n",
        "    # https://repository.upenn.edu/cgi/viewcontent.cgi?article=1039&context=ircs_reports\n",
        "    \"AD\": \"adverb\",\n",
        "    \"AS\": \"aspect marker\",\n",
        "    \"BA\": \"把 in ba-construction\",\n",
        "    # \"CD\": \"cardinal number\",\n",
        "    \"CS\": \"subordinating conjunction\",\n",
        "    \"DEC\": \"的 in a relative clause\",\n",
        "    \"DEG\": \"associative 的\",\n",
        "    \"DER\": \"得 in V-de const. and V-de-R\",\n",
        "    \"DEV\": \"地 before VP\",\n",
        "    \"ETC\": \"for words 等, 等等\",\n",
        "    # \"FW\": \"foreign words\"\n",
        "    \"IJ\": \"interjection\",\n",
        "    # \"JJ\": \"other noun-modifier\",\n",
        "    \"LB\": \"被 in long bei-const\",\n",
        "    \"LC\": \"localizer\",\n",
        "    \"M\": \"measure word\",\n",
        "    \"MSP\": \"other particle\",\n",
        "    # \"NN\": \"common noun\",\n",
        "    \"NR\": \"proper noun\",\n",
        "    \"NT\": \"temporal noun\",\n",
        "    \"OD\": \"ordinal number\",\n",
        "    \"ON\": \"onomatopoeia\",\n",
        "    \"P\": \"preposition excluding 把 and 被\",\n",
        "    \"PN\": \"pronoun\",\n",
        "    \"PU\": \"punctuation\",\n",
        "    \"SB\": \"被 in short bei-const\",\n",
        "    # \"SP\": \"sentence-final particle\",\n",
        "    \"VA\": \"predicative adjective\",\n",
        "    \"VC\": \"是 (copula)\",\n",
        "    \"VE\": \"有 as the main verb\",\n",
        "    \"VV\": \"other verb\",\n",
        "    # Noun chunks\n",
        "    \"NP\": \"noun phrase\",\n",
        "    \"PP\": \"prepositional phrase\",\n",
        "    \"VP\": \"verb phrase\",\n",
        "    \"ADVP\": \"adverb phrase\",\n",
        "    \"ADJP\": \"adjective phrase\",\n",
        "    \"SBAR\": \"subordinating conjunction\",\n",
        "    \"PRT\": \"particle\",\n",
        "    \"PNP\": \"prepositional noun phrase\",\n",
        "    # Dependency Labels (English)\n",
        "    # ClearNLP / Universal Dependencies\n",
        "    # https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n",
        "    \"acl\": \"clausal modifier of noun (adjectival clause)\",\n",
        "    \"acomp\": \"adjectival complement\",\n",
        "    \"advcl\": \"adverbial clause modifier\",\n",
        "    \"advmod\": \"adverbial modifier\",\n",
        "    \"agent\": \"agent\",\n",
        "    \"amod\": \"adjectival modifier\",\n",
        "    \"appos\": \"appositional modifier\",\n",
        "    \"attr\": \"attribute\",\n",
        "    \"aux\": \"auxiliary\",\n",
        "    \"auxpass\": \"auxiliary (passive)\",\n",
        "    \"case\": \"case marking\",\n",
        "    \"cc\": \"coordinating conjunction\",\n",
        "    \"ccomp\": \"clausal complement\",\n",
        "    \"clf\": \"classifier\",\n",
        "    \"complm\": \"complementizer\",\n",
        "    \"compound\": \"compound\",\n",
        "    \"conj\": \"conjunct\",\n",
        "    \"cop\": \"copula\",\n",
        "    \"csubj\": \"clausal subject\",\n",
        "    \"csubjpass\": \"clausal subject (passive)\",\n",
        "    \"dative\": \"dative\",\n",
        "    \"dep\": \"unclassified dependent\",\n",
        "    \"det\": \"determiner\",\n",
        "    \"discourse\": \"discourse element\",\n",
        "    \"dislocated\": \"dislocated elements\",\n",
        "    \"dobj\": \"direct object\",\n",
        "    \"expl\": \"expletive\",\n",
        "    \"fixed\": \"fixed multiword expression\",\n",
        "    \"flat\": \"flat multiword expression\",\n",
        "    \"goeswith\": \"goes with\",\n",
        "    \"hmod\": \"modifier in hyphenation\",\n",
        "    \"hyph\": \"hyphen\",\n",
        "    \"infmod\": \"infinitival modifier\",\n",
        "    \"intj\": \"interjection\",\n",
        "    \"iobj\": \"indirect object\",\n",
        "    \"list\": \"list\",\n",
        "    \"mark\": \"marker\",\n",
        "    \"meta\": \"meta modifier\",\n",
        "    \"neg\": \"negation modifier\",\n",
        "    \"nmod\": \"modifier of nominal\",\n",
        "    \"nn\": \"noun compound modifier\",\n",
        "    \"npadvmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"nsubj\": \"nominal subject\",\n",
        "    \"nsubjpass\": \"nominal subject (passive)\",\n",
        "    \"nounmod\": \"modifier of nominal\",\n",
        "    \"npmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"num\": \"number modifier\",\n",
        "    \"number\": \"number compound modifier\",\n",
        "    \"nummod\": \"numeric modifier\",\n",
        "    \"oprd\": \"object predicate\",\n",
        "    \"obj\": \"object\",\n",
        "    \"obl\": \"oblique nominal\",\n",
        "    \"orphan\": \"orphan\",\n",
        "    \"parataxis\": \"parataxis\",\n",
        "    \"partmod\": \"participal modifier\",\n",
        "    \"pcomp\": \"complement of preposition\",\n",
        "    \"pobj\": \"object of preposition\",\n",
        "    \"poss\": \"possession modifier\",\n",
        "    \"possessive\": \"possessive modifier\",\n",
        "    \"preconj\": \"pre-correlative conjunction\",\n",
        "    \"prep\": \"prepositional modifier\",\n",
        "    \"prt\": \"particle\",\n",
        "    \"punct\": \"punctuation\",\n",
        "    \"quantmod\": \"modifier of quantifier\",\n",
        "    \"rcmod\": \"relative clause modifier\",\n",
        "    \"relcl\": \"relative clause modifier\",\n",
        "    \"reparandum\": \"overridden disfluency\",\n",
        "    \"root\": \"root\",\n",
        "    \"ROOT\": \"root\",\n",
        "    \"vocative\": \"vocative\",\n",
        "    \"xcomp\": \"open clausal complement\",\n",
        "    # Dependency labels (German)\n",
        "    # TIGER Treebank\n",
        "    # http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/tiger_introduction.pdf\n",
        "    # currently missing: 'cc' (comparative complement) because of conflict\n",
        "    # with English labels\n",
        "    \"ac\": \"adpositional case marker\",\n",
        "    \"adc\": \"adjective component\",\n",
        "    \"ag\": \"genitive attribute\",\n",
        "    \"ams\": \"measure argument of adjective\",\n",
        "    \"app\": \"apposition\",\n",
        "    \"avc\": \"adverbial phrase component\",\n",
        "    \"cd\": \"coordinating conjunction\",\n",
        "    \"cj\": \"conjunct\",\n",
        "    \"cm\": \"comparative conjunction\",\n",
        "    \"cp\": \"complementizer\",\n",
        "    \"cvc\": \"collocational verb construction\",\n",
        "    \"da\": \"dative\",\n",
        "    \"dh\": \"discourse-level head\",\n",
        "    \"dm\": \"discourse marker\",\n",
        "    \"ep\": \"expletive es\",\n",
        "    \"hd\": \"head\",\n",
        "    \"ju\": \"junctor\",\n",
        "    \"mnr\": \"postnominal modifier\",\n",
        "    \"mo\": \"modifier\",\n",
        "    \"ng\": \"negation\",\n",
        "    \"nk\": \"noun kernel element\",\n",
        "    \"nmc\": \"numerical component\",\n",
        "    \"oa\": \"accusative object\",\n",
        "    \"oc\": \"clausal object\",\n",
        "    \"og\": \"genitive object\",\n",
        "    \"op\": \"prepositional object\",\n",
        "    \"par\": \"parenthetical element\",\n",
        "    \"pd\": \"predicate\",\n",
        "    \"pg\": \"phrasal genitive\",\n",
        "    \"ph\": \"placeholder\",\n",
        "    \"pm\": \"morphological particle\",\n",
        "    \"pnc\": \"proper noun component\",\n",
        "    \"rc\": \"relative clause\",\n",
        "    \"re\": \"repeated element\",\n",
        "    \"rs\": \"reported speech\",\n",
        "    \"sb\": \"subject\",\n",
        "    \"sbp\": \"passivized subject (PP)\",\n",
        "    \"sp\": \"subject or predicate\",\n",
        "    \"svp\": \"separable verb prefix\",\n",
        "    \"uc\": \"unit component\",\n",
        "    \"vo\": \"vocative\",\n",
        "    # Named Entity Recognition\n",
        "    # OntoNotes 5\n",
        "    # https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf\n",
        "    \"PERSON\": \"People, including fictional\",\n",
        "    \"NORP\": \"Nationalities or religious or political groups\",\n",
        "    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"ORG\": \"Companies, agencies, institutions, etc.\",\n",
        "    \"GPE\": \"Countries, cities, states\",\n",
        "    \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n",
        "    \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n",
        "    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
        "    \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n",
        "    \"LAW\": \"Named documents made into laws.\",\n",
        "    \"LANGUAGE\": \"Any named language\",\n",
        "    \"DATE\": \"Absolute or relative dates or periods\",\n",
        "    \"TIME\": \"Times smaller than a day\",\n",
        "    \"PERCENT\": 'Percentage, including \"%\"',\n",
        "    \"MONEY\": \"Monetary values, including unit\",\n",
        "    \"QUANTITY\": \"Measurements, as of weight or distance\",\n",
        "    \"ORDINAL\": '\"first\", \"second\", etc.',\n",
        "    \"CARDINAL\": \"Numerals that do not fall under another type\",\n",
        "    # Named Entity Recognition\n",
        "    # Wikipedia\n",
        "    # http://www.sciencedirect.com/science/article/pii/S0004370212000276\n",
        "    # https://pdfs.semanticscholar.org/5744/578cc243d92287f47448870bb426c66cc941.pdf\n",
        "    \"PER\": \"Named person or family.\",\n",
        "    \"MISC\": \"Miscellaneous entities, e.g. events, nationalities, products or works of art\",\n",
        "    # https://github.com/ltgoslo/norne\n",
        "    \"EVT\": \"Festivals, cultural events, sports events, weather phenomena, wars, etc.\",\n",
        "    \"PROD\": \"Product, i.e. artificially produced entities including speeches, radio shows, programming languages, contracts, laws and ideas\",\n",
        "    \"DRV\": \"Words (and phrases?) that are dervied from a name, but not a name in themselves, e.g. 'Oslo-mannen' ('the man from Oslo')\",\n",
        "    \"GPE_LOC\": \"Geo-political entity, with a locative sense, e.g. 'John lives in Spain'\",\n",
        "    \"GPE_ORG\": \"Geo-political entity, with an organisation sense, e.g. 'Spain declined to meet with Belgium'\",\n",
        "}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TF3OA6bD_7Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "dRDXRWuG_ywX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def pos_counter(row):\n",
        "  pos_vector = dict.fromkeys(GLOSSARY.keys(), 0)\n",
        "  doc = nlp(row)\n",
        "  for d in doc:\n",
        "    pos_vector[d.pos_] += 1\n",
        "  return np.asarray(list(pos_vector.values()))\n",
        "\n",
        "prompt_data['prompt_pos'] = prompt_data['prompt_text'].apply(lambda row: pos_counter(row))\n",
        "train_data['text_pos'] = train_data['corrected_text'].apply(lambda row: pos_counter(row))"
      ],
      "metadata": {
        "id": "gT21RYoBfHWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity Recognition"
      ],
      "metadata": {
        "id": "TfExlqNJ_7lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for matching same entities from prompt and summary\n",
        "def entity_map(row):\n",
        "  doc = nlp(row)\n",
        "  ents = [(e.text, e.label_) for e in doc.ents]\n",
        "  keys = [e.label_ for e in doc.ents]\n",
        "  map = {key:[] for key in keys}\n",
        "  for ent in ents:\n",
        "    map[ent[1]].append(ent[0])\n",
        "  return map\n",
        "\n",
        "prompt_data['prompt_entities'] = prompt_data['prompt_text'].apply(lambda row: entity_map(row))\n",
        "train_data['summary_entities'] = train_data['corrected_text'].apply(lambda row: entity_map(row))"
      ],
      "metadata": {
        "id": "G3YQLLGWAE_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different Words counter"
      ],
      "metadata": {
        "id": "oMM-UZfeLfYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def different_word_counter(row):\n",
        "  row = preprocessText(row)\n",
        "  words_list = []\n",
        "  for w in row.split(' '):\n",
        "    if w not in words_list:\n",
        "      words_list.append(w)\n",
        "\n",
        "  return len(words_list)\n"
      ],
      "metadata": {
        "id": "dIA0-widLw7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data[\"prompt_text_word_counter\"] = prompt_data['prompt_text'].apply(lambda row: different_word_counter(row))\n",
        "train_data['summary_word_counter'] = train_data['corrected_text'].apply(lambda row: different_word_counter(row))"
      ],
      "metadata": {
        "id": "DYQLyBo5Mbx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Da7_C7bVPC"
      },
      "outputs": [],
      "source": [
        "# @title Merging the dataset\n",
        "training_data = train_data.merge(prompt_data, on='prompt_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different words counter ratio"
      ],
      "metadata": {
        "id": "DlaJABOhOwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_counter_ratio(summary_word_counter, prompt_word_counter):\n",
        "  return summary_word_counter/prompt_word_counter"
      ],
      "metadata": {
        "id": "6T1zUnTlO69r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data['word_counter_ratio'] = training_data.apply(lambda row: word_counter_ratio(row[\"summary_word_counter\"], row[\"prompt_text_word_counter\"]), axis=1)"
      ],
      "metadata": {
        "id": "ZXiQZfjjO1R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity Comparison"
      ],
      "metadata": {
        "id": "R9z2BEPTdvfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_compare(d1, d2):\n",
        "    d1_keys = set(d1.keys())\n",
        "    d2_keys = set(d2.keys())\n",
        "    shared_keys = d1_keys.intersection(d2_keys)\n",
        "    same = 0\n",
        "    for key in shared_keys:\n",
        "      for val1 in d1[key]:\n",
        "        for val2 in d2[key]:\n",
        "          if val1 == val2:\n",
        "            same += 1\n",
        "    return same\n",
        "\n",
        "training_data['matching_entities'] = training_data.apply(lambda row: dict_compare(row.prompt_entities, row.summary_entities), axis=1)"
      ],
      "metadata": {
        "id": "RjrlE7eJX1AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams counting"
      ],
      "metadata": {
        "id": "ApT7ULnTdzsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIGPTQuEBi67"
      },
      "outputs": [],
      "source": [
        "def count_cooccurring_ngrams(text, prompt_text):\n",
        "    cooccurring_count = sum((text & prompt_text).values())\n",
        "    return cooccurring_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ussKX2DbKd"
      },
      "outputs": [],
      "source": [
        "for n in range(2, 5):\n",
        "\n",
        "  text_col = f\"{n}grams-text-count\"\n",
        "  corre_col = f\"{n}grams-correctedtext-count\"\n",
        "  prompt_col = f\"{n}grams-prompttext-count\"\n",
        "\n",
        "  new_col=f\"{n}grams-correct-count\"\n",
        "  training_data[new_col] = training_data.apply(lambda row: count_cooccurring_ngrams(row[text_col], row[corre_col]), axis=1)\n",
        "\n",
        "  new_col=f\"{n}grams-cooccurrence-count\"\n",
        "  training_data[new_col] = training_data.apply(lambda row: count_cooccurring_ngrams(row[corre_col], row[prompt_col]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Similarity"
      ],
      "metadata": {
        "id": "agQY4UwpgJQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1-Xcyj7ba9X"
      },
      "outputs": [],
      "source": [
        "def semantic_similarity(text1, text2, preprocess=False):\n",
        "  if preprocess:\n",
        "    text1 = preprocessText(text1)\n",
        "    text2 = preprocessText(text2)\n",
        "\n",
        "  embedding1 = nlp(text1)\n",
        "  embedding2 = nlp(text2)\n",
        "\n",
        "  return embedding1.similarity(embedding2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nUp40h_byvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bfd491-9eb1-41ec-b7ca-2eb9321daccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-34d691352d0d>:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  return embedding1.similarity(embedding2)\n"
          ]
        }
      ],
      "source": [
        "training_data[\"semantic_similarity\"] = training_data.apply(lambda row: semantic_similarity(row[\"corrected_text_preprocessed\"], row[\"prompt_text_preprocessed\"]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspelling finding and counting"
      ],
      "metadata": {
        "id": "a78QwzDJgdGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU6i4AmS5TMk"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "spell.word_frequency.load_words(token_list)\n",
        "\n",
        "def misspelled(text):\n",
        "  words = text.split()\n",
        "  misspelled = spell.unknown(words)\n",
        "  return misspelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUGMSkkP7otG"
      },
      "outputs": [],
      "source": [
        "training_data[\"text_misspelled_words\"] = training_data[\"text_pre_withstop\"].apply(lambda x: misspelled(x))\n",
        "training_data[\"corrected_misspelled_words\"] = training_data[\"corrected_text_pre_withstop\"].apply(lambda x: misspelled(x))\n",
        "\n",
        "training_data[\"text_misspelled_counter\"] = training_data[\"text_misspelled_words\"].apply(len)\n",
        "training_data[\"corrected_misspelled_counter\"] = training_data[\"corrected_misspelled_words\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text length ratio"
      ],
      "metadata": {
        "id": "qFeOZkksfxc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QinJ6t_lB46J"
      },
      "outputs": [],
      "source": [
        "training_data[\"length_ratio\"] = training_data[\"text_length\"] / training_data[\"prompt_text_length\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF score creation"
      ],
      "metadata": {
        "id": "gQ_p7hMUhmxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBYSZOYRJMBm"
      },
      "outputs": [],
      "source": [
        "def add_row(df1, df2, preprocess=False):\n",
        "  row = df2.unique().tolist()[0]\n",
        "  if preprocess:\n",
        "    row = preprocessText(row)\n",
        "  combined_data = pd.concat([pd.Series([row]),df1.loc[:]]).reset_index(drop=True) #append row on the head of the dataframe\n",
        "  return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahu8t9Uk80Rg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Group by 'prompt_id' and compute TF-IDF separately for each class\n",
        "tfidf_vectorizers = {}\n",
        "\n",
        "for class_id, group in training_data.groupby('prompt_id'):\n",
        "    #text_data = group['text'].apply(preprocessText)\n",
        "    text_data = group['corrected_text_preprocessed']\n",
        "\n",
        "    prompt_question_data = group['prompt_question']\n",
        "    prompt_title_data = group['prompt_title']\n",
        "    prompt_text_data = group['prompt_text_preprocessed']\n",
        "\n",
        "    # Concatenate the preprocessed data for TF-IDF calculation\n",
        "    combined_data = add_row(text_data, prompt_question_data, True)\n",
        "    combined_data = add_row(combined_data, prompt_title_data, True)\n",
        "    combined_data = add_row(combined_data, prompt_text_data)\n",
        "\n",
        "\n",
        "    # Compute TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_data)\n",
        "    tfidf_vectorizers[class_id] = {'vectorizer': tfidf_vectorizer, 'matrix': tfidf_matrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyNOYrhNbUw5"
      },
      "outputs": [],
      "source": [
        "import sympy\n",
        "\n",
        "karp_tfidf_scores = {}\n",
        "\n",
        "# Calculate TF-IDF scores for each document\n",
        "for class_id, group in training_data.groupby('prompt_id'):\n",
        "    tfidf_vectorizer = tfidf_vectorizers[class_id]['vectorizer']\n",
        "    tfidf_matrix = tfidf_vectorizers[class_id]['matrix']\n",
        "    tfidf_matrix = tfidf_matrix[3:] #remove first 3 rows f the matrix since they belongs to prompt_text, prompt_question, prompt_title\n",
        "\n",
        "    modulus = sympy.randprime(tfidf_matrix.shape[1]-100, tfidf_matrix.shape[1])\n",
        "\n",
        "    # Iterate through documents and calculate TF-IDF scores\n",
        "    for index, row in group.iterrows():\n",
        "        doc_tfidf = tfidf_matrix[index - group.index[0]].toarray()[0]\n",
        "\n",
        "        doc_tfidf = doc_tfidf[doc_tfidf>0]\n",
        "\n",
        "        # Calculate the average TF-IDF score for the document\n",
        "        #average_tfidf_score = sum(doc_tfidf) / len(doc_tfidf)\n",
        "\n",
        "        #average_tfidf_scores[index] = average_tfidf_score\n",
        "\n",
        "        gamma = 1e-2\n",
        "        single_tfidf_score = sum([t**(gamma*i) for i, t in enumerate(doc_tfidf)]) % modulus\n",
        "\n",
        "        karp_tfidf_scores[index] = single_tfidf_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDAAV1ZUghS_"
      },
      "outputs": [],
      "source": [
        "# Add the calculated average TF-IDF scores as a new column to the DataFrame\n",
        "training_data['karp_tfidf_scores'] = [karp_tfidf_scores[index] for index in training_data.index]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset normalization"
      ],
      "metadata": {
        "id": "-xrozNN_gmsE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u4lHQtTCJX8"
      },
      "outputs": [],
      "source": [
        "# normalize the data taking into consideration the prompt title\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "def normalize_col(training_data, col):\n",
        "  # Create a new DataFrame to store the normalized text length values\n",
        "  normalized_df = pd.DataFrame()\n",
        "\n",
        "  # Group by 'prompt_title' and apply the normalization separately for each group\n",
        "  for title, group in training_data.groupby('prompt_title'):\n",
        "      normalized_text_length = scaler.fit_transform(group[[col]])\n",
        "      new_name = \"normalized_\"+col\n",
        "      group[new_name] = normalized_text_length\n",
        "      normalized_df = pd.concat([normalized_df, group])\n",
        "  training_data = normalized_df.copy()\n",
        "  return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo1-rdosKNzl"
      },
      "outputs": [],
      "source": [
        "normalize_cols = [\"text_length\", \"text_misspelled_counter\", \"corrected_misspelled_counter\", \"2grams-cooccurrence-count\", \"2grams-correct-count\",\n",
        "                  \"3grams-cooccurrence-count\", \"3grams-correct-count\", \"4grams-cooccurrence-count\", \"4grams-correct-count\", \"karp_tfidf_scores\", \"summary_word_counter\"]\n",
        "for col in normalize_cols:\n",
        "  training_data = normalize_col(training_data, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaaSZoXW1Wsq"
      },
      "outputs": [],
      "source": [
        "normalize_cols.extend(['prompt_text_length'])\n",
        "del_columns.extend(normalize_cols)\n",
        "\n",
        "training_data.drop(columns=del_columns, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COF-n9uiSKWp"
      },
      "outputs": [],
      "source": [
        "training_data.rename(columns={\"normalized_karp_tfidf_scores\": \"karp_tfidf_scores\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "AQeDeHrkR8BE",
        "outputId": "a4eab66c-a66c-4217-b252-10a30473186b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     prompt_id                                               text   content  \\\n",
              "3099    3b9047  In Egypt, there were many occupations and soci...  3.128928   \n",
              "3100    3b9047  The highest class was Pharaohs these people we... -0.210614   \n",
              "3101    3b9047  The Egyptian society is really different from ...  0.205683   \n",
              "3102    3b9047  We have the gods and then Logan  and If Logan ... -1.547163   \n",
              "3103    3b9047  The social classes are different because they ... -0.066112   \n",
              "\n",
              "       wording                                 fixed_summary_text  \\\n",
              "3099  4.231226  In Egypt, there were many occupations and soci...   \n",
              "3100 -0.471415  The highest class was Pharaohs these people we...   \n",
              "3101  0.380538  The Egyptian society is really different from ...   \n",
              "3102 -1.461245  We have the gods and then Logan  and If Logan ...   \n",
              "3103 -0.715083  The social classes are different because they ...   \n",
              "\n",
              "                                         corrected_text  \\\n",
              "3099  In Egypt, there were many occupations and soci...   \n",
              "3100  The highest class was Pharaohs these people we...   \n",
              "3101  The Egyptian society is really different from ...   \n",
              "3102  We have the gods and then Logan  and If Logan ...   \n",
              "3103  The social classes are different because they ...   \n",
              "\n",
              "                            corrected_text_preprocessed  \\\n",
              "3099  egypt many occupation social class involved da...   \n",
              "3100  highest class pharaoh people god 2nd highest c...   \n",
              "3101  egyptian society really different society lear...   \n",
              "3102  god logan logan something wh know frank frank ...   \n",
              "3103  social class different different activity like...   \n",
              "\n",
              "                                      text_pre_withstop  \\\n",
              "3099  in egypt there were many occupations and socia...   \n",
              "3100  the highest class was pharaohs these people we...   \n",
              "3101  the egyptian society is really different from ...   \n",
              "3102  we have the gods and then logan and if logan d...   \n",
              "3103  the social classes are different because they ...   \n",
              "\n",
              "                            corrected_text_pre_withstop  \\\n",
              "3099  in egypt there were many occupations and socia...   \n",
              "3100  the highest class was pharaohs these people we...   \n",
              "3101  the egyptian society is really different from ...   \n",
              "3102  we have the gods and then logan and if logan d...   \n",
              "3103  the social classes are different because they ...   \n",
              "\n",
              "                                               text_pos  ...  \\\n",
              "3099  [15, 24, 10, 20, 0, 12, 22, 2, 56, 2, 10, 22, ...  ...   \n",
              "3100  [3, 1, 1, 4, 0, 0, 6, 0, 6, 0, 0, 0, 4, 5, 0, ...  ...   \n",
              "3101  [10, 9, 3, 7, 0, 2, 7, 0, 16, 2, 1, 9, 0, 5, 4...  ...   \n",
              "3102  [0, 0, 3, 1, 0, 4, 1, 0, 2, 0, 0, 5, 4, 1, 2, ...  ...   \n",
              "3103  [5, 13, 0, 5, 0, 3, 9, 1, 19, 1, 4, 4, 2, 7, 3...  ...   \n",
              "\n",
              "     normalized_text_misspelled_counter  \\\n",
              "3099                           0.076923   \n",
              "3100                           0.076923   \n",
              "3101                           0.076923   \n",
              "3102                           0.076923   \n",
              "3103                           0.000000   \n",
              "\n",
              "     normalized_corrected_misspelled_counter  \\\n",
              "3099                                0.166667   \n",
              "3100                                0.083333   \n",
              "3101                                0.000000   \n",
              "3102                                0.083333   \n",
              "3103                                0.000000   \n",
              "\n",
              "     normalized_2grams-cooccurrence-count normalized_2grams-correct-count  \\\n",
              "3099                             0.138889                        0.349381   \n",
              "3100                             0.015152                        0.017882   \n",
              "3101                             0.020202                        0.089409   \n",
              "3102                             0.002525                        0.019257   \n",
              "3103                             0.103535                        0.103164   \n",
              "\n",
              "     normalized_3grams-cooccurrence-count normalized_3grams-correct-count  \\\n",
              "3099                             0.051151                        0.355062   \n",
              "3100                             0.012788                        0.020804   \n",
              "3101                             0.000000                        0.091540   \n",
              "3102                             0.000000                        0.022191   \n",
              "3103                             0.089514                        0.108183   \n",
              "\n",
              "     normalized_4grams-cooccurrence-count  normalized_4grams-correct-count  \\\n",
              "3099                             0.020725                         0.360335   \n",
              "3100                             0.010363                         0.023743   \n",
              "3101                             0.000000                         0.093575   \n",
              "3102                             0.000000                         0.025140   \n",
              "3103                             0.088083                         0.113128   \n",
              "\n",
              "      karp_tfidf_scores  normalized_summary_word_counter  \n",
              "3099           0.956539                         0.477157  \n",
              "3100           0.156903                         0.035533  \n",
              "3101           0.460427                         0.111675  \n",
              "3102           0.079536                         0.015228  \n",
              "3103           0.537596                         0.126904  \n",
              "\n",
              "[5 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d179afda-6df6-42fd-80de-a46230c6df7e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>content</th>\n",
              "      <th>wording</th>\n",
              "      <th>fixed_summary_text</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_text_preprocessed</th>\n",
              "      <th>text_pre_withstop</th>\n",
              "      <th>corrected_text_pre_withstop</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>...</th>\n",
              "      <th>normalized_text_misspelled_counter</th>\n",
              "      <th>normalized_corrected_misspelled_counter</th>\n",
              "      <th>normalized_2grams-cooccurrence-count</th>\n",
              "      <th>normalized_2grams-correct-count</th>\n",
              "      <th>normalized_3grams-cooccurrence-count</th>\n",
              "      <th>normalized_3grams-correct-count</th>\n",
              "      <th>normalized_4grams-cooccurrence-count</th>\n",
              "      <th>normalized_4grams-correct-count</th>\n",
              "      <th>karp_tfidf_scores</th>\n",
              "      <th>normalized_summary_word_counter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>3.128928</td>\n",
              "      <td>4.231226</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>egypt many occupation social class involved da...</td>\n",
              "      <td>in egypt there were many occupations and socia...</td>\n",
              "      <td>in egypt there were many occupations and socia...</td>\n",
              "      <td>[15, 24, 10, 20, 0, 12, 22, 2, 56, 2, 10, 22, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.138889</td>\n",
              "      <td>0.349381</td>\n",
              "      <td>0.051151</td>\n",
              "      <td>0.355062</td>\n",
              "      <td>0.020725</td>\n",
              "      <td>0.360335</td>\n",
              "      <td>0.956539</td>\n",
              "      <td>0.477157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>-0.210614</td>\n",
              "      <td>-0.471415</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>highest class pharaoh people god 2nd highest c...</td>\n",
              "      <td>the highest class was pharaohs these people we...</td>\n",
              "      <td>the highest class was pharaohs these people we...</td>\n",
              "      <td>[3, 1, 1, 4, 0, 0, 6, 0, 6, 0, 0, 0, 4, 5, 0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.017882</td>\n",
              "      <td>0.012788</td>\n",
              "      <td>0.020804</td>\n",
              "      <td>0.010363</td>\n",
              "      <td>0.023743</td>\n",
              "      <td>0.156903</td>\n",
              "      <td>0.035533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3101</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The Egyptian society is really different from ...</td>\n",
              "      <td>0.205683</td>\n",
              "      <td>0.380538</td>\n",
              "      <td>The Egyptian society is really different from ...</td>\n",
              "      <td>The Egyptian society is really different from ...</td>\n",
              "      <td>egyptian society really different society lear...</td>\n",
              "      <td>the egyptian society is really different from ...</td>\n",
              "      <td>the egyptian society is really different from ...</td>\n",
              "      <td>[10, 9, 3, 7, 0, 2, 7, 0, 16, 2, 1, 9, 0, 5, 4...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.089409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.091540</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093575</td>\n",
              "      <td>0.460427</td>\n",
              "      <td>0.111675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3102</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>We have the gods and then Logan  and If Logan ...</td>\n",
              "      <td>-1.547163</td>\n",
              "      <td>-1.461245</td>\n",
              "      <td>We have the gods and then Logan  and If Logan ...</td>\n",
              "      <td>We have the gods and then Logan  and If Logan ...</td>\n",
              "      <td>god logan logan something wh know frank frank ...</td>\n",
              "      <td>we have the gods and then logan and if logan d...</td>\n",
              "      <td>we have the gods and then logan and if logan d...</td>\n",
              "      <td>[0, 0, 3, 1, 0, 4, 1, 0, 2, 0, 0, 5, 4, 1, 2, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.002525</td>\n",
              "      <td>0.019257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022191</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025140</td>\n",
              "      <td>0.079536</td>\n",
              "      <td>0.015228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3103</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The social classes are different because they ...</td>\n",
              "      <td>-0.066112</td>\n",
              "      <td>-0.715083</td>\n",
              "      <td>The social classes are different because they ...</td>\n",
              "      <td>The social classes are different because they ...</td>\n",
              "      <td>social class different different activity like...</td>\n",
              "      <td>the social classes are different because they ...</td>\n",
              "      <td>the social classes are different because they ...</td>\n",
              "      <td>[5, 13, 0, 5, 0, 3, 9, 1, 19, 1, 4, 4, 2, 7, 3...</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.103535</td>\n",
              "      <td>0.103164</td>\n",
              "      <td>0.089514</td>\n",
              "      <td>0.108183</td>\n",
              "      <td>0.088083</td>\n",
              "      <td>0.113128</td>\n",
              "      <td>0.537596</td>\n",
              "      <td>0.126904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d179afda-6df6-42fd-80de-a46230c6df7e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d179afda-6df6-42fd-80de-a46230c6df7e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d179afda-6df6-42fd-80de-a46230c6df7e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5cf88717-3620-4428-97c7-6cf3471c7635\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5cf88717-3620-4428-97c7-6cf3471c7635')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5cf88717-3620-4428-97c7-6cf3471c7635 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUvd9oYqB-tt",
        "outputId": "8a17a2bd-3807-4c45-c6fb-e5e82c3c43e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-3ecd97e2210c>:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  training_data.corrwith(training_data[\"content\"])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content                                    1.000000\n",
              "wording                                    0.751380\n",
              "prompt_text_word_counter                  -0.027778\n",
              "word_counter_ratio                         0.801223\n",
              "matching_entities                          0.337624\n",
              "semantic_similarity                        0.537417\n",
              "length_ratio                               0.777299\n",
              "normalized_text_length                     0.781834\n",
              "normalized_text_misspelled_counter         0.275465\n",
              "normalized_corrected_misspelled_counter    0.194798\n",
              "normalized_2grams-cooccurrence-count       0.538317\n",
              "normalized_2grams-correct-count            0.778925\n",
              "normalized_3grams-cooccurrence-count       0.388556\n",
              "normalized_3grams-correct-count            0.773245\n",
              "normalized_4grams-cooccurrence-count       0.338966\n",
              "normalized_4grams-correct-count            0.768883\n",
              "karp_tfidf_scores                          0.837049\n",
              "normalized_summary_word_counter            0.786006\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "training_data.corrwith(training_data[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh4c8BlhCHwy",
        "outputId": "42f53242-fe6f-4c18-dbb5-74eec2badfed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-360ae61d465e>:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  training_data.corrwith(training_data[\"wording\"])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content                                    0.751380\n",
              "wording                                    1.000000\n",
              "prompt_text_word_counter                  -0.134943\n",
              "word_counter_ratio                         0.559691\n",
              "matching_entities                          0.290207\n",
              "semantic_similarity                        0.288848\n",
              "length_ratio                               0.546496\n",
              "normalized_text_length                     0.556227\n",
              "normalized_text_misspelled_counter         0.218678\n",
              "normalized_corrected_misspelled_counter    0.172347\n",
              "normalized_2grams-cooccurrence-count       0.206662\n",
              "normalized_2grams-correct-count            0.552565\n",
              "normalized_3grams-cooccurrence-count       0.051939\n",
              "normalized_3grams-correct-count            0.546936\n",
              "normalized_4grams-cooccurrence-count       0.013728\n",
              "normalized_4grams-correct-count            0.544816\n",
              "karp_tfidf_scores                          0.566751\n",
              "normalized_summary_word_counter            0.533349\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "training_data.corrwith(training_data[\"wording\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2_aoRzlIjN9"
      },
      "outputs": [],
      "source": [
        "training_data.to_csv('dataset.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}