{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sopralapanca/CommonLitChallenge/blob/main/Correction%2BPOS%2BNER%2BPrepareDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IDA3TWbPR4d"
      },
      "source": [
        "LAVORO PER SARDE:\n",
        "1. Aggiungere altri todo\n",
        "2. Imparare ad essere umile\n",
        "3. Tifare il Campobasso\n",
        "\n",
        "TODO: CONTROLLARE I PARAGRAFI\n",
        "\n",
        "1. Controllare se non ci sono whitespace dopo gli .\\n Maiuscola\n",
        "2. Score value spacy per identificare la similarità, potremmo usarla per paragrafi diversi.\n",
        "\n",
        "TODO: CONTROLLARE I NOMI PROPRI\n",
        "1. Si potrebbe effettuare il lower() prima di aggiungere le parole allo spellchecker sperando che ci siano i nomi propri necessari alla stesura del riassunto all'interno del prompt\n",
        "2. Controllare i nomi propri facendo ner e non tenerli in considerazione per eventuali errori, ma confrontarli con eventuali nomi propri all'interno dei prompt (selezionare i soggetti che iniziano per le stesse lettere e poi calcolarcisi l'edit distance ==> wording, entro un certo range viene considerato match e si salva una feature per il numero di match con i prompt ==> content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muui6d4mCFm4"
      },
      "source": [
        "## Data importing and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weCgm4nI56zm",
        "outputId": "d36a999f-9b6a-4482-c1de-b7450c866546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting primesieve<=2.0\n",
            "  Downloading primesieve-2.0.0.tar.gz (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symspellpy\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Building wheels for collected packages: primesieve, editdistpy\n",
            "  Building wheel for primesieve (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for primesieve: filename=primesieve-2.0.0-cp310-cp310-linux_x86_64.whl size=2301898 sha256=03ec4651a0f0beaebf696cd991cec53befdda8e86b908147a2263d14b6706a31\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/a3/c8/2cc8545f43b7eda7ab3cd69b62bf8881a0c9440daecf25326e\n",
            "  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=190883 sha256=5f6e733f2a0e20827382a134e0f045ee4cf4b20f554462c250d6bc5e9fd6b463\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\n",
            "Successfully built primesieve editdistpy\n",
            "Installing collected packages: primesieve, pyspellchecker, editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.3 primesieve-2.0.0 pyspellchecker-0.7.2 symspellpy-6.7.7\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy pyspellchecker symspellpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-GRGqEVbG2D",
        "outputId": "7e2b7f93-b4f7-48c0-80f8-646798c1887e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-15 18:22:31.726976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-15 18:22:33.767010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-09-15 18:22:36.591423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-15 18:22:36.592084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-09-15 18:22:36.592352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNgQ83Og83ke",
        "outputId": "182e7100-7737-4266-efc3-4101f08341d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CommonLitChallenge'...\n",
            "remote: Enumerating objects: 263, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 263 (delta 62), reused 67 (delta 50), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (263/263), 19.80 MiB | 10.24 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/Sopralapanca/CommonLitChallenge.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q3bk8Dmz-GWE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "summaries_train_path = \"/content/CommonLitChallenge/data/summaries_train.csv\"\n",
        "prompt_train_path = \"/content/CommonLitChallenge/data/prompts_train.csv\"\n",
        "\n",
        "train_data = pd.read_csv(summaries_train_path, sep=',', index_col=0)\n",
        "prompt_data = pd.read_csv(prompt_train_path, sep=',', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "draRz0ddB_G_"
      },
      "source": [
        "## Language Processing\n",
        "\n",
        "> Definition of the Glossary \\\\\n",
        "> Creation of the word list \\\\\n",
        "> Language Model Class construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TF3OA6bD_7Q6"
      },
      "outputs": [],
      "source": [
        "# @title Glossary def.\n",
        "GLOSSARY = {\n",
        "    \"ADJ\": \"adjective\",\n",
        "    \"ADP\": \"adposition\",\n",
        "    \"ADV\": \"adverb\",\n",
        "    \"AUX\": \"auxiliary\",\n",
        "    \"CONJ\": \"conjunction\",\n",
        "    \"CCONJ\": \"coordinating conjunction\",\n",
        "    \"DET\": \"determiner\",\n",
        "    \"INTJ\": \"interjection\",\n",
        "    \"NOUN\": \"noun\",\n",
        "    \"NUM\": \"numeral\",\n",
        "    \"PART\": \"particle\",\n",
        "    \"PRON\": \"pronoun\",\n",
        "    \"PROPN\": \"proper noun\",\n",
        "    \"PUNCT\": \"punctuation\",\n",
        "    \"SCONJ\": \"subordinating conjunction\",\n",
        "    \"SYM\": \"symbol\",\n",
        "    \"VERB\": \"verb\",\n",
        "    \"X\": \"other\",\n",
        "    \"EOL\": \"end of line\",\n",
        "    \"SPACE\": \"space\",\n",
        "    \".\": \"punctuation mark, sentence closer\",\n",
        "    \",\": \"punctuation mark, comma\",\n",
        "    \"-LRB-\": \"left round bracket\",\n",
        "    \"-RRB-\": \"right round bracket\",\n",
        "    \"``\": \"opening quotation mark\",\n",
        "    '\"\"': \"closing quotation mark\",\n",
        "    \"''\": \"closing quotation mark\",\n",
        "    \":\": \"punctuation mark, colon or ellipsis\",\n",
        "    \"$\": \"symbol, currency\",\n",
        "    \"#\": \"symbol, number sign\",\n",
        "    \"AFX\": \"affix\",\n",
        "    \"CC\": \"conjunction, coordinating\",\n",
        "    \"CD\": \"cardinal number\",\n",
        "    \"DT\": \"determiner\",\n",
        "    \"EX\": \"existential there\",\n",
        "    \"FW\": \"foreign word\",\n",
        "    \"HYPH\": \"punctuation mark, hyphen\",\n",
        "    \"IN\": \"conjunction, subordinating or preposition\",\n",
        "    \"JJ\": \"adjective (English), other noun-modifier (Chinese)\",\n",
        "    \"JJR\": \"adjective, comparative\",\n",
        "    \"JJS\": \"adjective, superlative\",\n",
        "    \"LS\": \"list item marker\",\n",
        "    \"MD\": \"verb, modal auxiliary\",\n",
        "    \"NIL\": \"missing tag\",\n",
        "    \"NN\": \"noun, singular or mass\",\n",
        "    \"NNP\": \"noun, proper singular\",\n",
        "    \"NNPS\": \"noun, proper plural\",\n",
        "    \"NNS\": \"noun, plural\",\n",
        "    \"PDT\": \"predeterminer\",\n",
        "    \"POS\": \"possessive ending\",\n",
        "    \"PRP\": \"pronoun, personal\",\n",
        "    \"PRP$\": \"pronoun, possessive\",\n",
        "    \"RB\": \"adverb\",\n",
        "    \"RBR\": \"adverb, comparative\",\n",
        "    \"RBS\": \"adverb, superlative\",\n",
        "    \"RP\": \"adverb, particle\",\n",
        "    \"TO\": 'infinitival \"to\"',\n",
        "    \"UH\": \"interjection\",\n",
        "    \"VB\": \"verb, base form\",\n",
        "    \"VBD\": \"verb, past tense\",\n",
        "    \"VBG\": \"verb, gerund or present participle\",\n",
        "    \"VBN\": \"verb, past participle\",\n",
        "    \"VBP\": \"verb, non-3rd person singular present\",\n",
        "    \"VBZ\": \"verb, 3rd person singular present\",\n",
        "    \"WDT\": \"wh-determiner\",\n",
        "    \"WP\": \"wh-pronoun, personal\",\n",
        "    \"WP$\": \"wh-pronoun, possessive\",\n",
        "    \"WRB\": \"wh-adverb\",\n",
        "    \"SP\": \"space (English), sentence-final particle (Chinese)\",\n",
        "    \"ADD\": \"email\",\n",
        "    \"NFP\": \"superfluous punctuation\",\n",
        "    \"GW\": \"additional word in multi-word expression\",\n",
        "    \"XX\": \"unknown\",\n",
        "    \"BES\": 'auxiliary \"be\"',\n",
        "    \"HVS\": 'forms of \"have\"',\n",
        "    \"_SP\": \"whitespace\",\n",
        "    \"$(\": \"other sentence-internal punctuation mark\",\n",
        "    \"$,\": \"comma\",\n",
        "    \"$.\": \"sentence-final punctuation mark\",\n",
        "    \"ADJA\": \"adjective, attributive\",\n",
        "    \"ADJD\": \"adjective, adverbial or predicative\",\n",
        "    \"APPO\": \"postposition\",\n",
        "    \"APPR\": \"preposition; circumposition left\",\n",
        "    \"APPRART\": \"preposition with article\",\n",
        "    \"APZR\": \"circumposition right\",\n",
        "    \"ART\": \"definite or indefinite article\",\n",
        "    \"CARD\": \"cardinal number\",\n",
        "    \"FM\": \"foreign language material\",\n",
        "    \"ITJ\": \"interjection\",\n",
        "    \"KOKOM\": \"comparative conjunction\",\n",
        "    \"KON\": \"coordinate conjunction\",\n",
        "    \"KOUI\": 'subordinate conjunction with \"zu\" and infinitive',\n",
        "    \"KOUS\": \"subordinate conjunction with sentence\",\n",
        "    \"NE\": \"proper noun\",\n",
        "    \"NNE\": \"proper noun\",\n",
        "    \"PAV\": \"pronominal adverb\",\n",
        "    \"PROAV\": \"pronominal adverb\",\n",
        "    \"PDAT\": \"attributive demonstrative pronoun\",\n",
        "    \"PDS\": \"substituting demonstrative pronoun\",\n",
        "    \"PIAT\": \"attributive indefinite pronoun without determiner\",\n",
        "    \"PIDAT\": \"attributive indefinite pronoun with determiner\",\n",
        "    \"PIS\": \"substituting indefinite pronoun\",\n",
        "    \"PPER\": \"non-reflexive personal pronoun\",\n",
        "    \"PPOSAT\": \"attributive possessive pronoun\",\n",
        "    \"PPOSS\": \"substituting possessive pronoun\",\n",
        "    \"PRELAT\": \"attributive relative pronoun\",\n",
        "    \"PRELS\": \"substituting relative pronoun\",\n",
        "    \"PRF\": \"reflexive personal pronoun\",\n",
        "    \"PTKA\": \"particle with adjective or adverb\",\n",
        "    \"PTKANT\": \"answer particle\",\n",
        "    \"PTKNEG\": \"negative particle\",\n",
        "    \"PTKVZ\": \"separable verbal particle\",\n",
        "    \"PTKZU\": '\"zu\" before infinitive',\n",
        "    \"PWAT\": \"attributive interrogative pronoun\",\n",
        "    \"PWAV\": \"adverbial interrogative or relative pronoun\",\n",
        "    \"PWS\": \"substituting interrogative pronoun\",\n",
        "    \"TRUNC\": \"word remnant\",\n",
        "    \"VAFIN\": \"finite verb, auxiliary\",\n",
        "    \"VAIMP\": \"imperative, auxiliary\",\n",
        "    \"VAINF\": \"infinitive, auxiliary\",\n",
        "    \"VAPP\": \"perfect participle, auxiliary\",\n",
        "    \"VMFIN\": \"finite verb, modal\",\n",
        "    \"VMINF\": \"infinitive, modal\",\n",
        "    \"VMPP\": \"perfect participle, modal\",\n",
        "    \"VVFIN\": \"finite verb, full\",\n",
        "    \"VVIMP\": \"imperative, full\",\n",
        "    \"VVINF\": \"infinitive, full\",\n",
        "    \"VVIZU\": 'infinitive with \"zu\", full',\n",
        "    \"VVPP\": \"perfect participle, full\",\n",
        "    \"XY\": \"non-word containing non-letter\",\n",
        "    \"IJ\": \"interjection\",\n",
        "    \"NR\": \"proper noun\",\n",
        "    \"NT\": \"temporal noun\",\n",
        "    \"OD\": \"ordinal number\",\n",
        "    \"ON\": \"onomatopoeia\",\n",
        "    \"PN\": \"pronoun\",\n",
        "    \"PU\": \"punctuation\",\n",
        "    \"NP\": \"noun phrase\",\n",
        "    \"PP\": \"prepositional phrase\",\n",
        "    \"VP\": \"verb phrase\",\n",
        "    \"ADVP\": \"adverb phrase\",\n",
        "    \"ADJP\": \"adjective phrase\",\n",
        "    \"SBAR\": \"subordinating conjunction\",\n",
        "    \"PRT\": \"particle\",\n",
        "    \"PNP\": \"prepositional noun phrase\",\n",
        "    \"acl\": \"clausal modifier of noun (adjectival clause)\",\n",
        "    \"acomp\": \"adjectival complement\",\n",
        "    \"advcl\": \"adverbial clause modifier\",\n",
        "    \"advmod\": \"adverbial modifier\",\n",
        "    \"agent\": \"agent\",\n",
        "    \"amod\": \"adjectival modifier\",\n",
        "    \"appos\": \"appositional modifier\",\n",
        "    \"attr\": \"attribute\",\n",
        "    \"aux\": \"auxiliary\",\n",
        "    \"auxpass\": \"auxiliary (passive)\",\n",
        "    \"case\": \"case marking\",\n",
        "    \"cc\": \"coordinating conjunction\",\n",
        "    \"ccomp\": \"clausal complement\",\n",
        "    \"clf\": \"classifier\",\n",
        "    \"complm\": \"complementizer\",\n",
        "    \"compound\": \"compound\",\n",
        "    \"conj\": \"conjunct\",\n",
        "    \"cop\": \"copula\",\n",
        "    \"csubj\": \"clausal subject\",\n",
        "    \"csubjpass\": \"clausal subject (passive)\",\n",
        "    \"dative\": \"dative\",\n",
        "    \"dep\": \"unclassified dependent\",\n",
        "    \"det\": \"determiner\",\n",
        "    \"discourse\": \"discourse element\",\n",
        "    \"dislocated\": \"dislocated elements\",\n",
        "    \"dobj\": \"direct object\",\n",
        "    \"expl\": \"expletive\",\n",
        "    \"fixed\": \"fixed multiword expression\",\n",
        "    \"flat\": \"flat multiword expression\",\n",
        "    \"goeswith\": \"goes with\",\n",
        "    \"hmod\": \"modifier in hyphenation\",\n",
        "    \"hyph\": \"hyphen\",\n",
        "    \"infmod\": \"infinitival modifier\",\n",
        "    \"intj\": \"interjection\",\n",
        "    \"iobj\": \"indirect object\",\n",
        "    \"list\": \"list\",\n",
        "    \"mark\": \"marker\",\n",
        "    \"meta\": \"meta modifier\",\n",
        "    \"neg\": \"negation modifier\",\n",
        "    \"nmod\": \"modifier of nominal\",\n",
        "    \"nn\": \"noun compound modifier\",\n",
        "    \"npadvmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"nsubj\": \"nominal subject\",\n",
        "    \"nsubjpass\": \"nominal subject (passive)\",\n",
        "    \"nounmod\": \"modifier of nominal\",\n",
        "    \"npmod\": \"noun phrase as adverbial modifier\",\n",
        "    \"num\": \"number modifier\",\n",
        "    \"number\": \"number compound modifier\",\n",
        "    \"nummod\": \"numeric modifier\",\n",
        "    \"oprd\": \"object predicate\",\n",
        "    \"obj\": \"object\",\n",
        "    \"obl\": \"oblique nominal\",\n",
        "    \"orphan\": \"orphan\",\n",
        "    \"parataxis\": \"parataxis\",\n",
        "    \"partmod\": \"participal modifier\",\n",
        "    \"pcomp\": \"complement of preposition\",\n",
        "    \"pobj\": \"object of preposition\",\n",
        "    \"poss\": \"possession modifier\",\n",
        "    \"possessive\": \"possessive modifier\",\n",
        "    \"preconj\": \"pre-correlative conjunction\",\n",
        "    \"prep\": \"prepositional modifier\",\n",
        "    \"prt\": \"particle\",\n",
        "    \"punct\": \"punctuation\",\n",
        "    \"quantmod\": \"modifier of quantifier\",\n",
        "    \"rcmod\": \"relative clause modifier\",\n",
        "    \"relcl\": \"relative clause modifier\",\n",
        "    \"reparandum\": \"overridden disfluency\",\n",
        "    \"root\": \"root\",\n",
        "    \"ROOT\": \"root\",\n",
        "    \"vocative\": \"vocative\",\n",
        "    \"xcomp\": \"open clausal complement\",\n",
        "    \"ac\": \"adpositional case marker\",\n",
        "    \"adc\": \"adjective component\",\n",
        "    \"ag\": \"genitive attribute\",\n",
        "    \"ams\": \"measure argument of adjective\",\n",
        "    \"app\": \"apposition\",\n",
        "    \"avc\": \"adverbial phrase component\",\n",
        "    \"cd\": \"coordinating conjunction\",\n",
        "    \"cj\": \"conjunct\",\n",
        "    \"cm\": \"comparative conjunction\",\n",
        "    \"cp\": \"complementizer\",\n",
        "    \"cvc\": \"collocational verb construction\",\n",
        "    \"da\": \"dative\",\n",
        "    \"dh\": \"discourse-level head\",\n",
        "    \"dm\": \"discourse marker\",\n",
        "    \"ep\": \"expletive es\",\n",
        "    \"hd\": \"head\",\n",
        "    \"ju\": \"junctor\",\n",
        "    \"mnr\": \"postnominal modifier\",\n",
        "    \"mo\": \"modifier\",\n",
        "    \"ng\": \"negation\",\n",
        "    \"nk\": \"noun kernel element\",\n",
        "    \"nmc\": \"numerical component\",\n",
        "    \"oa\": \"accusative object\",\n",
        "    \"oc\": \"clausal object\",\n",
        "    \"og\": \"genitive object\",\n",
        "    \"op\": \"prepositional object\",\n",
        "    \"par\": \"parenthetical element\",\n",
        "    \"pd\": \"predicate\",\n",
        "    \"pg\": \"phrasal genitive\",\n",
        "    \"ph\": \"placeholder\",\n",
        "    \"pm\": \"morphological particle\",\n",
        "    \"pnc\": \"proper noun component\",\n",
        "    \"rc\": \"relative clause\",\n",
        "    \"re\": \"repeated element\",\n",
        "    \"rs\": \"reported speech\",\n",
        "    \"sb\": \"subject\",\n",
        "    \"sbp\": \"passivized subject (PP)\",\n",
        "    \"sp\": \"subject or predicate\",\n",
        "    \"svp\": \"separable verb prefix\",\n",
        "    \"uc\": \"unit component\",\n",
        "    \"vo\": \"vocative\",\n",
        "    \"PERSON\": \"People, including fictional\",\n",
        "    \"NORP\": \"Nationalities or religious or political groups\",\n",
        "    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n",
        "    \"ORG\": \"Companies, agencies, institutions, etc.\",\n",
        "    \"GPE\": \"Countries, cities, states\",\n",
        "    \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n",
        "    \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n",
        "    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
        "    \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n",
        "    \"LAW\": \"Named documents made into laws.\",\n",
        "    \"LANGUAGE\": \"Any named language\",\n",
        "    \"DATE\": \"Absolute or relative dates or periods\",\n",
        "    \"TIME\": \"Times smaller than a day\",\n",
        "    \"PERCENT\": 'Percentage, including \"%\"',\n",
        "    \"MONEY\": \"Monetary values, including unit\",\n",
        "    \"QUANTITY\": \"Measurements, as of weight or distance\",\n",
        "    \"ORDINAL\": '\"first\", \"second\", etc.',\n",
        "    \"CARDINAL\": \"Numerals that do not fall under another type\",\n",
        "    \"PER\": \"Named person or family.\",\n",
        "    \"MISC\": \"Miscellaneous entities, e.g. events, nationalities, products or works of art\",\n",
        "    \"EVT\": \"Festivals, cultural events, sports events, weather phenomena, wars, etc.\",\n",
        "    \"PROD\": \"Product, i.e. artificially produced entities including speeches, radio shows, programming languages, contracts, laws and ideas\",\n",
        "    \"DRV\": \"Words (and phrases?) that are dervied from a name, but not a name in themselves, e.g. 'Oslo-mannen' ('the man from Oslo')\",\n",
        "    \"GPE_LOC\": \"Geo-political entity, with a locative sense, e.g. 'John lives in Spain'\",\n",
        "    \"GPE_ORG\": \"Geo-political entity, with an organisation sense, e.g. 'Spain declined to meet with Belgium'\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkz-UMcvBvst"
      },
      "source": [
        "### Creating the list of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W8XMtG9s8_c8"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "word_list = []\n",
        "for prompt, query, title in zip(prompt_data.prompt_text.tolist(), prompt_data.prompt_question.tolist(), prompt_data.prompt_title.tolist()):\n",
        "  word_list.append(prompt.replace('\\n', ' ').split())\n",
        "  word_list.append(query.replace('\\n', ' ').split())\n",
        "  word_list.append(title.replace('\\n', ' ').split())\n",
        "token_list = list(itertools.chain(*word_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULONxuzDB7f_"
      },
      "source": [
        "### Language Model from Spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "drUzHk2u75tH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell\n",
        "\n",
        "class LanguageMagician():\n",
        "  def __init__(self) -> None:\n",
        "    # Initialization of Spacy\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.gamma = 3e-3\n",
        "    self.modulus = 541\n",
        "\n",
        "    # Initialization of SymSpell\n",
        "    self.sym_spell = SymSpell(max_dictionary_edit_distance=4, prefix_length=7)\n",
        "    dictionary_path = pkg_resources.resource_filename(\n",
        "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        "    )\n",
        "    bigram_path = pkg_resources.resource_filename(\n",
        "        \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
        "    )\n",
        "    # term_index is the column of the term and count_index is the\n",
        "    # column of the term frequency\n",
        "    self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "    self.sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "  def init_vocabulary_(self, words):\n",
        "    vector_data = {word: np.random.uniform(-1, 1, (300,)) for word in words}\n",
        "    vocab = self.nlp.vocab\n",
        "    for word, vector in vector_data.items():\n",
        "        vocab.set_vector(word, vector)\n",
        "\n",
        "  def init_sym_vocabulary(self, words):\n",
        "    #open text file\n",
        "    text_file = open(\"vocabulary.txt\", \"a\")\n",
        "    #write string to file\n",
        "    for s in words:\n",
        "      text_file.write(s)\n",
        "\n",
        "    #close file\n",
        "    text_file.close()\n",
        "\n",
        "    corpus_path = 'vocabulary.txt'\n",
        "    try:\n",
        "      result = self.sym_spell.create_dictionary(corpus_path)\n",
        "    except:\n",
        "      raise(f'error the dictionary creation has returned {result}')\n",
        "\n",
        "\n",
        "  def correct(self, row):\n",
        "    # max edit distance per lookup (per single word, not per whole input string)\n",
        "    suggestions = self.sym_spell.lookup_compound(row, max_edit_distance=3,\n",
        "                                                #  transfer_casing=True\n",
        "                                                 )\n",
        "    # display suggestion term, edit distance, and term frequency\n",
        "    return suggestions[0].term\n",
        "\n",
        "  def pos_scorer(self, row):\n",
        "    pos_vector = dict.fromkeys(GLOSSARY.keys(), 0)\n",
        "    doc = self.nlp(row)\n",
        "    for d in doc:\n",
        "      pos_vector[d.pos_] += 1\n",
        "    single_pos_score = 0\n",
        "    np_pos = np.asarray(list(pos_vector.values()))\n",
        "    for i, pos in enumerate(np_pos):\n",
        "      single_pos_score += int(pos)**(self.gamma*i)\n",
        "    return single_pos_score % self.modulus\n",
        "\n",
        "  def entity_map(self, row):\n",
        "    doc = self.nlp(row)\n",
        "    ents = [(e.text, e.label_) for e in doc.ents]\n",
        "    keys = [e.label_ for e in doc.ents]\n",
        "    map = {key:[] for key in keys}\n",
        "    for ent in ents:\n",
        "      map[ent[1]].append(ent[0])\n",
        "    return map\n",
        "\n",
        "  def entity_scorer(self, row):\n",
        "    entity_vector = dict.fromkeys(GLOSSARY.keys(), 0)\n",
        "    for k, v in row.items():\n",
        "      entity_vector[k] = len(v)\n",
        "    single_ent_score = 0\n",
        "    np_ent = np.asarray(list(entity_vector.values()))\n",
        "    for i, ent in enumerate(np_ent):\n",
        "      single_ent_score += int(ent)**(self.gamma*i)\n",
        "    return single_ent_score % self.modulus\n",
        "\n",
        "  def semantic_similarity(self, text1, text2):\n",
        "    embedding1 = self.nlp(text1)\n",
        "    embedding2 = self.nlp(text2)\n",
        "    return embedding1.similarity(embedding2)\n",
        "\n",
        "\n",
        "\n",
        "lang_wizard = LanguageMagician()\n",
        "lang_wizard.init_vocabulary_(token_list)\n",
        "lang_wizard.init_sym_vocabulary(token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BdDSaEhj9JMV"
      },
      "outputs": [],
      "source": [
        "train_data[\"corrected_text\"] = train_data['text'].apply(lambda x: lang_wizard.correct(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpl5SG5yCQ1r"
      },
      "source": [
        "## Text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtNSmpUJKFs4",
        "outputId": "8c7d933a-50d7-4fc9-a109-e4e49df04dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocessText(text, removal=True):\n",
        "    # replace newline with space\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace('\\r', ' ')\n",
        "\n",
        "    # Normalize spaces around punctuation marks\n",
        "    text = re.sub(r\"[^A-Za-z0-9']\", r' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Replace curly apostrophe with straight single quote\n",
        "    text = text.replace('’', \"'\")\n",
        "\n",
        "    # Normalize spaces around punctuation marks\n",
        "    text = text.strip()\n",
        "\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "\n",
        "    if removal:\n",
        "      # split text\n",
        "      words = text.split()\n",
        "\n",
        "      # stop word removal\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "\n",
        "      # stemming\n",
        "      # words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "      # lemmatization\n",
        "      words = [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "      text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess the text\n",
        "prompt_data[\"prompt_text_preprocessed\"] =   prompt_data[\"prompt_text\"].apply(lambda x: preprocessText(x))\n",
        "train_data[\"corrected_text_preprocessed\"] = train_data[\"corrected_text\"].apply(lambda x: preprocessText(x))\n",
        "\n",
        "train_data[\"text_pre_withstop\"] =           train_data[\"text\"].apply(lambda x: preprocessText(x, False))\n",
        "train_data[\"corrected_text_pre_withstop\"] = train_data[\"corrected_text\"].apply(lambda x: preprocessText(x, False))\n",
        "# Count the length of text\n",
        "prompt_data[\"prompt_text_length\"] = prompt_data[\"prompt_text\"].apply(len)\n",
        "train_data[\"text_length\"] =         train_data[\"corrected_text\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0YUWjNOCK7b"
      },
      "source": [
        "### N-grams finding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAGW8zyP8BVL",
        "outputId": "6ba892ed-a22e-4a12-8fb3-0cfcee21c882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the required resources for tokenization\n",
        "\n",
        "def count_ngrams(text, n):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  ngram_counts = Counter(ngrams(words, n))\n",
        "  return ngram_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "We1dhK1SBIvB"
      },
      "outputs": [],
      "source": [
        "del_columns = []\n",
        "\n",
        "for n in range(2,5):\n",
        "  col = f\"{n}grams-prompttext-count\"\n",
        "  prompt_data[col] = prompt_data.apply(lambda row: count_ngrams(row[\"prompt_text\"], n), axis=1)\n",
        "  del_columns.append(col)\n",
        "\n",
        "  col = f\"{n}grams-correctedtext-count\"\n",
        "  train_data[col] = train_data.apply(lambda row: count_ngrams(row[\"corrected_text\"], n), axis=1)\n",
        "  del_columns.append(col)\n",
        "\n",
        "  col = f\"{n}grams-text-count\"\n",
        "  train_data[col] = train_data.apply(lambda row: count_ngrams(row[\"text\"], n), axis=1)\n",
        "  del_columns.append(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCdvELSBfI3z"
      },
      "source": [
        "### POS counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gT21RYoBfHWQ"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# def karp_scoring(df, col):\n",
        "#   def karp_pos(row):\n",
        "#     pos_vector = dict.fromkeys(GLOSSARY.keys(), 0)\n",
        "#     doc = nlp(row)\n",
        "#     for d in doc:\n",
        "#       pos_vector[d.pos_] += 1\n",
        "#     modulus = sympy.randprime(len(row)-10, len(row)+10)\n",
        "#     gamma = 3e-2\n",
        "#     single_pos_score = 0\n",
        "#     np_pos = np.asarray(list(pos_vector.values()))\n",
        "#     for i, pos in enumerate(np_pos):\n",
        "#       single_pos_score += int(pos)**(gamma*i)\n",
        "#     return single_pos_score % modulus\n",
        "\n",
        "train_data['text_pos'] = train_data['corrected_text'].apply(lambda row: lang_wizard.pos_scorer(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfExlqNJ_7lp"
      },
      "source": [
        "### Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G3YQLLGWAE_p"
      },
      "outputs": [],
      "source": [
        "# Function for matching same entities from prompt and summary\n",
        "# def entity_map(row):\n",
        "#   doc = nlp(row)\n",
        "#   ents = [(e.text, e.label_) for e in doc.ents]\n",
        "#   keys = [e.label_ for e in doc.ents]\n",
        "#   map = {key:[] for key in keys}\n",
        "#   for ent in ents:\n",
        "#     map[ent[1]].append(ent[0])\n",
        "#   return map\n",
        "\n",
        "prompt_data['prompt_entities'] = prompt_data['prompt_text'].apply(lambda row: lang_wizard.entity_map(row))\n",
        "train_data['summary_entities'] = train_data['corrected_text'].apply(lambda row: lang_wizard.entity_map(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different Words counter"
      ],
      "metadata": {
        "id": "oMM-UZfeLfYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def different_word_counter(row):\n",
        "  row = preprocessText(row)\n",
        "  words_list = []\n",
        "  for w in row.split(' '):\n",
        "    if w not in words_list:\n",
        "      words_list.append(w)\n",
        "\n",
        "  return len(words_list)\n"
      ],
      "metadata": {
        "id": "dIA0-widLw7r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data[\"prompt_text_word_counter\"] = prompt_data['prompt_text'].apply(lambda row: different_word_counter(row))\n",
        "train_data['summary_word_counter'] = train_data['corrected_text'].apply(lambda row: different_word_counter(row))"
      ],
      "metadata": {
        "id": "DYQLyBo5Mbx6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E9Da7_C7bVPC"
      },
      "outputs": [],
      "source": [
        "# @title Merging the dataset\n",
        "training_data = train_data.merge(prompt_data, on='prompt_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Different words counter ratio"
      ],
      "metadata": {
        "id": "DlaJABOhOwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_counter_ratio(summary_word_counter, prompt_word_counter):\n",
        "  return summary_word_counter/prompt_word_counter"
      ],
      "metadata": {
        "id": "6T1zUnTlO69r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data['word_counter_ratio'] = training_data.apply(lambda row: word_counter_ratio(row[\"summary_word_counter\"], row[\"prompt_text_word_counter\"]), axis=1)"
      ],
      "metadata": {
        "id": "ZXiQZfjjO1R7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9z2BEPTdvfj"
      },
      "source": [
        "### Entity Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RjrlE7eJX1AD"
      },
      "outputs": [],
      "source": [
        "def dict_compare(d1, d2):\n",
        "    d1_keys = set(d1.keys())\n",
        "    d2_keys = set(d2.keys())\n",
        "    shared_keys = d1_keys.intersection(d2_keys)\n",
        "    same = 0\n",
        "    for key in shared_keys:\n",
        "      for val1 in d1[key]:\n",
        "        for val2 in d2[key]:\n",
        "          if val1 == val2:\n",
        "            same += 1\n",
        "    return same\n",
        "\n",
        "training_data['matching_entities'] = training_data.apply(lambda row: dict_compare(row.prompt_entities, row.summary_entities), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApT7ULnTdzsM"
      },
      "source": [
        "### N-grams counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rIGPTQuEBi67"
      },
      "outputs": [],
      "source": [
        "def count_cooccurring_ngrams(text, prompt_text):\n",
        "    cooccurring_count = sum((text & prompt_text).values())\n",
        "    return cooccurring_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c3ussKX2DbKd"
      },
      "outputs": [],
      "source": [
        "for n in range(2, 5):\n",
        "\n",
        "  text_col = f\"{n}grams-text-count\"\n",
        "  corre_col = f\"{n}grams-correctedtext-count\"\n",
        "  prompt_col = f\"{n}grams-prompttext-count\"\n",
        "\n",
        "  new_col=f\"{n}grams-correct-count\"\n",
        "  training_data[new_col] = training_data.apply(lambda row: count_cooccurring_ngrams(row[text_col], row[corre_col]), axis=1)\n",
        "\n",
        "  new_col=f\"{n}grams-cooccurrence-count\"\n",
        "  training_data[new_col] = training_data.apply(lambda row: count_cooccurring_ngrams(row[corre_col], row[prompt_col]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agQY4UwpgJQM"
      },
      "source": [
        "### Semantic Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "J1-Xcyj7ba9X"
      },
      "outputs": [],
      "source": [
        "# def semantic_similarity(text1, text2, preprocess=False):\n",
        "#   if preprocess:\n",
        "#     text1 = preprocessText(text1)\n",
        "#     text2 = preprocessText(text2)\n",
        "\n",
        "#   embedding1 = nlp(text1)\n",
        "#   embedding2 = nlp(text2)\n",
        "\n",
        "#   return embedding1.similarity(embedding2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8nUp40h_byvY"
      },
      "outputs": [],
      "source": [
        "training_data[\"semantic_similarity\"] = training_data.apply(lambda row: lang_wizard.semantic_similarity(row[\"corrected_text_preprocessed\"], row[\"prompt_text_preprocessed\"]), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78QwzDJgdGK"
      },
      "source": [
        "### Misspelling finding and counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rU6i4AmS5TMk"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "spell.word_frequency.load_words(token_list)\n",
        "\n",
        "def misspelled(text):\n",
        "  words = text.split()\n",
        "  misspelled = spell.unknown(words)\n",
        "  return misspelled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EUGMSkkP7otG"
      },
      "outputs": [],
      "source": [
        "training_data[\"text_misspelled_words\"] = training_data[\"text_pre_withstop\"].apply(lambda x: misspelled(x))\n",
        "\n",
        "training_data[\"text_misspelled_counter\"] = training_data[\"text_misspelled_words\"].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFeOZkksfxc-"
      },
      "source": [
        "### Text length ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QinJ6t_lB46J"
      },
      "outputs": [],
      "source": [
        "training_data[\"length_ratio\"] = training_data[\"text_length\"] / training_data[\"prompt_text_length\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ_p7hMUhmxK"
      },
      "source": [
        "### TF-IDF score creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BBYSZOYRJMBm"
      },
      "outputs": [],
      "source": [
        "def add_row(df1, df2, preprocess=False):\n",
        "  row = df2.unique().tolist()[0]\n",
        "  if preprocess:\n",
        "    row = preprocessText(row)\n",
        "  combined_data = pd.concat([pd.Series([row]),df1.loc[:]]).reset_index(drop=True) #append row on the head of the dataframe\n",
        "  return combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ahu8t9Uk80Rg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Group by 'prompt_id' and compute TF-IDF separately for each class\n",
        "tfidf_vectorizers = {}\n",
        "\n",
        "for class_id, group in training_data.groupby('prompt_id'):\n",
        "    #text_data = group['text'].apply(preprocessText)\n",
        "    text_data = group['corrected_text_preprocessed']\n",
        "\n",
        "    prompt_question_data = group['prompt_question']\n",
        "    prompt_title_data = group['prompt_title']\n",
        "    prompt_text_data = group['prompt_text_preprocessed']\n",
        "\n",
        "    # Concatenate the preprocessed data for TF-IDF calculation\n",
        "    combined_data = add_row(text_data, prompt_question_data, True)\n",
        "    combined_data = add_row(combined_data, prompt_title_data, True)\n",
        "    combined_data = add_row(combined_data, prompt_text_data)\n",
        "\n",
        "\n",
        "    # Compute TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_data)\n",
        "    tfidf_vectorizers[class_id] = {'vectorizer': tfidf_vectorizer, 'matrix': tfidf_matrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EyNOYrhNbUw5"
      },
      "outputs": [],
      "source": [
        "karp_tfidf_scores = {}\n",
        "\n",
        "# Calculate TF-IDF scores for each document\n",
        "for class_id, group in training_data.groupby('prompt_id'):\n",
        "    tfidf_vectorizer = tfidf_vectorizers[class_id]['vectorizer']\n",
        "    tfidf_matrix = tfidf_vectorizers[class_id]['matrix']\n",
        "    tfidf_matrix = tfidf_matrix[3:] #remove first 3 rows f the matrix since they belongs to prompt_text, prompt_question, prompt_title\n",
        "\n",
        "    modulus = 541\n",
        "\n",
        "    # Iterate through documents and calculate TF-IDF scores\n",
        "    for index, row in group.iterrows():\n",
        "        doc_tfidf = tfidf_matrix[index - group.index[0]].toarray()[0]\n",
        "\n",
        "        doc_tfidf = doc_tfidf[doc_tfidf>0]\n",
        "\n",
        "        # Calculate the average TF-IDF score for the document\n",
        "        #average_tfidf_score = sum(doc_tfidf) / len(doc_tfidf)\n",
        "\n",
        "        #average_tfidf_scores[index] = average_tfidf_score\n",
        "\n",
        "        gamma = 1e-2\n",
        "        single_tfidf_score = sum([t**(gamma*i) for i, t in enumerate(doc_tfidf)]) % modulus\n",
        "\n",
        "        karp_tfidf_scores[index] = single_tfidf_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HDAAV1ZUghS_"
      },
      "outputs": [],
      "source": [
        "# Add the calculated average TF-IDF scores as a new column to the DataFrame\n",
        "training_data['karp_tfidf_scores'] = [karp_tfidf_scores[index] for index in training_data.index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xrozNN_gmsE"
      },
      "source": [
        "## Dataset normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4u4lHQtTCJX8"
      },
      "outputs": [],
      "source": [
        "# normalize the data taking into consideration the prompt title\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "def normalize_col(training_data, col):\n",
        "  # Create a new DataFrame to store the normalized text length values\n",
        "  normalized_df = pd.DataFrame()\n",
        "\n",
        "  # Group by 'prompt_title' and apply the normalization separately for each group\n",
        "  for title, group in training_data.groupby('prompt_title'):\n",
        "      normalized_text_length = scaler.fit_transform(group[[col]])\n",
        "      new_name = \"normalized_\"+col\n",
        "      group[new_name] = normalized_text_length\n",
        "      normalized_df = pd.concat([normalized_df, group])\n",
        "  training_data = normalized_df.copy()\n",
        "  return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Lo1-rdosKNzl"
      },
      "outputs": [],
      "source": [
        "normalize_cols = [\"text_length\", \"text_misspelled_counter\",\n",
        "                  \"2grams-cooccurrence-count\", \"2grams-correct-count\",\n",
        "                  \"3grams-cooccurrence-count\", \"3grams-correct-count\",\n",
        "                  \"4grams-cooccurrence-count\", \"4grams-correct-count\",\n",
        "                  \"karp_tfidf_scores\"]\n",
        "for col in normalize_cols:\n",
        "  training_data = normalize_col(training_data, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TaaSZoXW1Wsq"
      },
      "outputs": [],
      "source": [
        "normalize_cols.extend(['prompt_text_length'])\n",
        "del_columns.extend(normalize_cols)\n",
        "\n",
        "training_data.drop(columns=del_columns, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "AQeDeHrkR8BE",
        "outputId": "31bd869e-3d27-4a25-8a61-4dd61890833e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     prompt_id                                               text   content  \\\n",
              "3099    3b9047  In Egypt, there were many occupations and soci...  3.128928   \n",
              "3100    3b9047  The highest class was Pharaohs these people we... -0.210614   \n",
              "3101    3b9047  The Egyptian society is really different from ...  0.205683   \n",
              "3102    3b9047  We have the gods and then Logan  and If Logan ... -1.547163   \n",
              "3103    3b9047  The social classes are different because they ... -0.066112   \n",
              "\n",
              "       wording                                     corrected_text  \\\n",
              "3099  4.231226  in egypt there were many occupations and socia...   \n",
              "3100 -0.471415  the highest class was pharaohs these people we...   \n",
              "3101  0.380538  the egyptian society is really different from ...   \n",
              "3102 -1.461245  we have the gods and then logan and if logan d...   \n",
              "3103 -0.715083  the social classes are different because they ...   \n",
              "\n",
              "                            corrected_text_preprocessed  \\\n",
              "3099  egypt many occupation social class involved da...   \n",
              "3100  highest class pharaoh people god highest class...   \n",
              "3101  egyptian society really different society lear...   \n",
              "3102  god logan logan something know frank frank go ...   \n",
              "3103  social class different different activity like...   \n",
              "\n",
              "                                      text_pre_withstop  \\\n",
              "3099  in egypt there were many occupations and socia...   \n",
              "3100  the highest class was pharaohs these people we...   \n",
              "3101  the egyptian society is really different from ...   \n",
              "3102  we have the gods and then logan and if logan d...   \n",
              "3103  the social classes are different because they ...   \n",
              "\n",
              "                            corrected_text_pre_withstop   text_pos  \\\n",
              "3099  in egypt there were many occupations and socia...  14.716971   \n",
              "3100  the highest class was pharaohs these people we...  10.132417   \n",
              "3101  the egyptian society is really different from ...  11.401449   \n",
              "3102  we have the gods and then logan and if logan d...  10.224557   \n",
              "3103  the social classes are different because they ...  12.421335   \n",
              "\n",
              "                             summary_entities  ...  length_ratio  \\\n",
              "3099  {'GPE': ['egypt'], 'CARDINAL': ['one']}  ...      0.401622   \n",
              "3100                                       {}  ...      0.046260   \n",
              "3101                   {'NORP': ['egyptian']}  ...      0.134575   \n",
              "3102                                       {}  ...      0.039051   \n",
              "3103                    {'CARDINAL': ['one']}  ...      0.129168   \n",
              "\n",
              "     normalized_text_length normalized_text_misspelled_counter  \\\n",
              "3099               0.328944                           0.076923   \n",
              "3100               0.011019                           0.076923   \n",
              "3101               0.090030                           0.076923   \n",
              "3102               0.004569                           0.076923   \n",
              "3103               0.085192                           0.000000   \n",
              "\n",
              "     normalized_2grams-cooccurrence-count normalized_2grams-correct-count  \\\n",
              "3099                             0.110320                        0.376299   \n",
              "3100                             0.021352                        0.029106   \n",
              "3101                             0.014235                        0.126819   \n",
              "3102                             0.003559                        0.024948   \n",
              "3103                             0.113879                        0.147609   \n",
              "\n",
              "     normalized_3grams-cooccurrence-count  normalized_3grams-correct-count  \\\n",
              "3099                             0.051282                         0.389163   \n",
              "3100                             0.017094                         0.024631   \n",
              "3101                             0.000000                         0.133005   \n",
              "3102                             0.000000                         0.017241   \n",
              "3103                             0.102564                         0.160099   \n",
              "\n",
              "      normalized_4grams-cooccurrence-count  normalized_4grams-correct-count  \\\n",
              "3099                              0.036082                         0.356021   \n",
              "3100                              0.015464                         0.015707   \n",
              "3101                              0.000000                         0.123037   \n",
              "3102                              0.000000                         0.013089   \n",
              "3103                              0.103093                         0.154450   \n",
              "\n",
              "      normalized_karp_tfidf_scores  \n",
              "3099                      0.954013  \n",
              "3100                      0.132983  \n",
              "3101                      0.423854  \n",
              "3102                      0.052231  \n",
              "3103                      0.538122  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a575efd-ec29-4763-b714-c66a72c5051d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>content</th>\n",
              "      <th>wording</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>corrected_text_preprocessed</th>\n",
              "      <th>text_pre_withstop</th>\n",
              "      <th>corrected_text_pre_withstop</th>\n",
              "      <th>text_pos</th>\n",
              "      <th>summary_entities</th>\n",
              "      <th>...</th>\n",
              "      <th>length_ratio</th>\n",
              "      <th>normalized_text_length</th>\n",
              "      <th>normalized_text_misspelled_counter</th>\n",
              "      <th>normalized_2grams-cooccurrence-count</th>\n",
              "      <th>normalized_2grams-correct-count</th>\n",
              "      <th>normalized_3grams-cooccurrence-count</th>\n",
              "      <th>normalized_3grams-correct-count</th>\n",
              "      <th>normalized_4grams-cooccurrence-count</th>\n",
              "      <th>normalized_4grams-correct-count</th>\n",
              "      <th>normalized_karp_tfidf_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>In Egypt, there were many occupations and soci...</td>\n",
              "      <td>3.128928</td>\n",
              "      <td>4.231226</td>\n",
              "      <td>in egypt there were many occupations and socia...</td>\n",
              "      <td>egypt many occupation social class involved da...</td>\n",
              "      <td>in egypt there were many occupations and socia...</td>\n",
              "      <td>in egypt there were many occupations and socia...</td>\n",
              "      <td>14.716971</td>\n",
              "      <td>{'GPE': ['egypt'], 'CARDINAL': ['one']}</td>\n",
              "      <td>...</td>\n",
              "      <td>0.401622</td>\n",
              "      <td>0.328944</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.110320</td>\n",
              "      <td>0.376299</td>\n",
              "      <td>0.051282</td>\n",
              "      <td>0.389163</td>\n",
              "      <td>0.036082</td>\n",
              "      <td>0.356021</td>\n",
              "      <td>0.954013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3100</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The highest class was Pharaohs these people we...</td>\n",
              "      <td>-0.210614</td>\n",
              "      <td>-0.471415</td>\n",
              "      <td>the highest class was pharaohs these people we...</td>\n",
              "      <td>highest class pharaoh people god highest class...</td>\n",
              "      <td>the highest class was pharaohs these people we...</td>\n",
              "      <td>the highest class was pharaohs these people we...</td>\n",
              "      <td>10.132417</td>\n",
              "      <td>{}</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046260</td>\n",
              "      <td>0.011019</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.029106</td>\n",
              "      <td>0.017094</td>\n",
              "      <td>0.024631</td>\n",
              "      <td>0.015464</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.132983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3101</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The Egyptian society is really different from ...</td>\n",
              "      <td>0.205683</td>\n",
              "      <td>0.380538</td>\n",
              "      <td>the egyptian society is really different from ...</td>\n",
              "      <td>egyptian society really different society lear...</td>\n",
              "      <td>the egyptian society is really different from ...</td>\n",
              "      <td>the egyptian society is really different from ...</td>\n",
              "      <td>11.401449</td>\n",
              "      <td>{'NORP': ['egyptian']}</td>\n",
              "      <td>...</td>\n",
              "      <td>0.134575</td>\n",
              "      <td>0.090030</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.014235</td>\n",
              "      <td>0.126819</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.123037</td>\n",
              "      <td>0.423854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3102</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>We have the gods and then Logan  and If Logan ...</td>\n",
              "      <td>-1.547163</td>\n",
              "      <td>-1.461245</td>\n",
              "      <td>we have the gods and then logan and if logan d...</td>\n",
              "      <td>god logan logan something know frank frank go ...</td>\n",
              "      <td>we have the gods and then logan and if logan d...</td>\n",
              "      <td>we have the gods and then logan and if logan d...</td>\n",
              "      <td>10.224557</td>\n",
              "      <td>{}</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039051</td>\n",
              "      <td>0.004569</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.024948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013089</td>\n",
              "      <td>0.052231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3103</th>\n",
              "      <td>3b9047</td>\n",
              "      <td>The social classes are different because they ...</td>\n",
              "      <td>-0.066112</td>\n",
              "      <td>-0.715083</td>\n",
              "      <td>the social classes are different because they ...</td>\n",
              "      <td>social class different different activity like...</td>\n",
              "      <td>the social classes are different because they ...</td>\n",
              "      <td>the social classes are different because they ...</td>\n",
              "      <td>12.421335</td>\n",
              "      <td>{'CARDINAL': ['one']}</td>\n",
              "      <td>...</td>\n",
              "      <td>0.129168</td>\n",
              "      <td>0.085192</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113879</td>\n",
              "      <td>0.147609</td>\n",
              "      <td>0.102564</td>\n",
              "      <td>0.160099</td>\n",
              "      <td>0.103093</td>\n",
              "      <td>0.154450</td>\n",
              "      <td>0.538122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a575efd-ec29-4763-b714-c66a72c5051d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a575efd-ec29-4763-b714-c66a72c5051d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a575efd-ec29-4763-b714-c66a72c5051d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e903695-8d5d-4aa6-8403-57bfb82e7f8d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e903695-8d5d-4aa6-8403-57bfb82e7f8d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e903695-8d5d-4aa6-8403-57bfb82e7f8d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUvd9oYqB-tt",
        "outputId": "14ddf558-90d1-4788-d25f-a1c5028f15fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-3ecd97e2210c>:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  training_data.corrwith(training_data[\"content\"])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content                                 1.000000\n",
              "wording                                 0.751380\n",
              "text_pos                                0.581775\n",
              "summary_word_counter                    0.791634\n",
              "prompt_text_word_counter               -0.027778\n",
              "word_counter_ratio                      0.802038\n",
              "matching_entities                       0.275606\n",
              "semantic_similarity                     0.452823\n",
              "length_ratio                            0.777919\n",
              "normalized_text_length                  0.780823\n",
              "normalized_text_misspelled_counter      0.275465\n",
              "normalized_2grams-cooccurrence-count    0.535491\n",
              "normalized_2grams-correct-count         0.765712\n",
              "normalized_3grams-cooccurrence-count    0.388423\n",
              "normalized_3grams-correct-count         0.749101\n",
              "normalized_4grams-cooccurrence-count    0.338160\n",
              "normalized_4grams-correct-count         0.711367\n",
              "normalized_karp_tfidf_scores            0.837687\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "training_data.corrwith(training_data[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh4c8BlhCHwy",
        "outputId": "fe869776-d26a-4552-fbfe-d1b3cb3b6e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-360ae61d465e>:1: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  training_data.corrwith(training_data[\"wording\"])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content                                 0.751380\n",
              "wording                                 1.000000\n",
              "text_pos                                0.451932\n",
              "summary_word_counter                    0.516272\n",
              "prompt_text_word_counter               -0.134943\n",
              "word_counter_ratio                      0.560617\n",
              "matching_entities                       0.209562\n",
              "semantic_similarity                     0.161091\n",
              "length_ratio                            0.548927\n",
              "normalized_text_length                  0.556560\n",
              "normalized_text_misspelled_counter      0.218678\n",
              "normalized_2grams-cooccurrence-count    0.188779\n",
              "normalized_2grams-correct-count         0.548939\n",
              "normalized_3grams-cooccurrence-count    0.035004\n",
              "normalized_3grams-correct-count         0.538261\n",
              "normalized_4grams-cooccurrence-count   -0.001188\n",
              "normalized_4grams-correct-count         0.517875\n",
              "normalized_karp_tfidf_scores            0.566616\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "training_data.corrwith(training_data[\"wording\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "h2_aoRzlIjN9"
      },
      "outputs": [],
      "source": [
        "training_data.to_csv('dataset.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}