# -*- coding: utf-8 -*-
"""Copy of RegressionBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GTBcks4LbQRHd_FQeEYCe93HdxQYKks
"""
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from spacy.lang.en import English
import string
import spacy
from torch.utils.data import Dataset, DataLoader
from transformers import BertPreTrainedModel, BertModel
from transformers import AutoConfig, AutoTokenizer
import torch

from sklearn.model_selection import train_test_split
from transformers import AutoModelForSequenceClassification,AutoTokenizer

from torch import nn

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import DebertaModel
from transformers import DebertaPreTrainedModel
from tqdm import tqdm, trange
from accelerate import Accelerator
import os
import gc

print(os.getcwd())
summaries_train_path = "https://raw.githubusercontent.com/Sopralapanca/CommonLitChallenge/main/data/summaries_train.csv"
prompt_train_path = "https://raw.githubusercontent.com/Sopralapanca/CommonLitChallenge/main/data/prompts_train.csv"

summaries_test_path = "https://raw.githubusercontent.com/Sopralapanca/CommonLitChallenge/main/data/summaries_test.csv"
prompt_test_path = "https://raw.githubusercontent.com/Sopralapanca/CommonLitChallenge/main/data/prompts_test.csv"

train_data = pd.read_csv(summaries_train_path, sep=',', index_col=0)
prompt_data = pd.read_csv(prompt_train_path, sep=',', index_col=0)

all_strings = train_data["text"].tolist() + prompt_data["prompt_question"].tolist() + prompt_data["prompt_title"].tolist() + prompt_data["prompt_text"].tolist()
len(all_strings)

training_data = train_data.merge(prompt_data, on='prompt_id')

training_data.head()

summary_word_count = training_data['text'].apply(lambda x: len(x.split()))
prompt_word_count = training_data['prompt_text'].apply(lambda x: len(x.split()))



# Length of the summaries made by the students
fig = plt.figure(figsize=[10,7])
sns.histplot(summary_word_count, color=sns.xkcd_rgb['greenish teal'], kde=True)
plt.show()

summary_word_count.max()

# Sorting the DataFrame based on the length of strings in the 'text' column
df_sorted = training_data.sort_values(by='text', key=lambda x: x.str.len(), ascending=False)
df_sorted.head()

# length of the full chapter
fig = plt.figure(figsize=[10,7])
sns.histplot(prompt_word_count, kde=True)
plt.show()

"""# Preprocessing dataset"""



def preprocessText(text):
    try:
        # replace newline with space
        text = text.replace("\n", " ")
        # split text
        words = text.split()
        # convert to lowercase
        words = [word.lower() for word in words]
        # remove punctuations
        table = str.maketrans('', '', string.punctuation)
        stripped = [w.translate(table) for w in words]
        # select only alphabetic characters
        words = [word for word in stripped if word.isalpha()]
        # stop word removal
        stop_words = spacy.lang.en.stop_words.STOP_WORDS
        words = [w for w in words if not w in stop_words]
        # return pre-processed paragraph text
        text = ' '.join(words)
        return text
    except:
        return text

string_columns = ["text","prompt_question","prompt_title","prompt_text"]

for col in string_columns:
  # apply preprocessText function to each text column in the dfTrain dataframe
  training_data[col] = training_data[col].apply(lambda x: preprocessText(x))

training_data.head()

summary_word_count = training_data['text'].apply(lambda x: len(x.split()))
prompt_word_count = training_data['prompt_text'].apply(lambda x: len(x.split()))

# Length of the summaries made by the students
fig = plt.figure(figsize=[10,7])
sns.histplot(summary_word_count, color=sns.xkcd_rgb['greenish teal'], kde=True)
plt.show()

fig = plt.figure(figsize=[10,7])
sns.histplot(prompt_word_count, kde=True)
plt.show()

summary_word_count.max()

prompt_word_count.max()

"""# PyTorch Dataset"""




class CommonLitDataset(Dataset):

    def __init__(self, data, maxlen, tokenizer, input_cols, target_cols):
        #Store the contents of the file in a pandas dataframe
        self.df = data.reset_index()
        #Initialize the tokenizer for the desired transformer model
        self.tokenizer = tokenizer
        #Maximum length of the tokens list to keep all the sequences of fixed size
        self.maxlen = maxlen
        #list of input columns
        self.input_cols = input_cols
        #list of target columns
        self.target_cols = target_cols

    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, index):
        #Select the sentence and label at the specified index in the data frame
        tokens=[]
        for col in self.input_cols:
          temp_tokens = self.tokenizer.tokenize(self.df.loc[index, col])
          tokens = tokens + temp_tokens + ['[SEP]']
        print("Length Token")
        print(len(tokens))
        try:
            target = self.df.loc[index, self.target_cols]
        except Exception as e:
           raise e
        #Preprocess the text to be suitable for the transformer
        tokens = ['[CLS]'] + tokens
        if len(tokens) < self.maxlen:
            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))]
        else:
            print("Too long string")
            tokens = tokens[:self.maxlen-1] + ['[SEP]']

        #Obtain the indices of the tokens in the BERT Vocabulary
        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        input_ids = torch.tensor(input_ids)
        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones
        attention_mask = (input_ids != 0).long()
        target = torch.tensor(target, dtype=torch.float32)

        return input_ids, attention_mask, target

train, test = train_test_split(training_data,  test_size=0.2, shuffle=True)
train, validation = train_test_split(train,  test_size=0.2, shuffle=True)

print(train.shape, validation.shape, test.shape)

"""# Debert v3"""



MODEL_NAME= 'microsoft/deberta-v3-base'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# Model Configurations
MAX_LEN = 1024 #summary_word_count.max()
BATCH_SIZE = 1

# Configuration loaded from AutoConfig
config = AutoConfig.from_pretrained(MODEL_NAME)

input_cols=["text","prompt_question","prompt_text"]
target_cols=["content", "wording"]
train_set = CommonLitDataset(data=train, maxlen=MAX_LEN, tokenizer=tokenizer, input_cols=input_cols, target_cols=target_cols)
valid_set = CommonLitDataset(data=validation, maxlen=MAX_LEN, tokenizer=tokenizer, input_cols=input_cols, target_cols=target_cols)
test_set = CommonLitDataset(data=test, maxlen=MAX_LEN, tokenizer=tokenizer, input_cols=input_cols, target_cols=target_cols)

train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)
valid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)
test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)

print(len(train_loader))

train_loader

sample = next(iter(train_loader))
sample

# batch of sentences, all the sentences must be MAX_LEN length because they are padded
sample[0].shape

# batch of attention masks
sample[1].shape

sample[1]

# target
sample[2].shape

"""# PyTorch Model"""



class RMSELoss(nn.Module):
    def __init__(self, eps=1e-6):
        super().__init__()
        self.mse = nn.MSELoss()
        self.eps = eps

    def forward(self, yhat, y):
        loss = torch.sqrt(self.mse(yhat, y) + self.eps)
        return loss


class MCRMSELoss(nn.Module):
    def __init__(self, num_scored=2):
        super().__init__()
        self.rmse = RMSELoss()
        self.num_scored = num_scored

    def forward(self, yhat, y):
        score = 0
        for i in range(self.num_scored):
            score += self.rmse(yhat[:, i], y[:, i]) / self.num_scored

        return score


class BertRegresser(DebertaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = DebertaModel(config)
        #The output layer that takes the [CLS] representation and gives an output
        self.cls_layer1 = nn.Linear(config.hidden_size,128)
        self.relu1 = nn.ReLU()
        self.ff1 = nn.Linear(128,2)

    def forward(self, input_ids, attention_mask):
        #Feed the input to Bert model to obtain contextualized representations
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        #Obtain the representations of [CLS] heads
        logits = outputs.last_hidden_state[:,0,:]
        output = self.cls_layer1(logits)
        output = self.relu1(output)
        output = self.ff1(output)
        return output

def evaluate(model, criterion, dataloader, device):
    model.eval()
    mean_acc, mean_loss, count = 0, 0, 0

    with torch.no_grad():
        for input_ids, attention_mask, target in (dataloader):

            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)
            output = model(input_ids, attention_mask)

            mean_loss += criterion(output, target).item()
            #mean_loss += get_rmse(output, target.type_as(output)).item()
            count += 1

    return mean_loss/count


accelerator = Accelerator(gradient_accumulation_steps=2)
def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):
    best_acc = 0
    for epoch in trange(epochs, desc="Epoch"):
        model.train()
        train_loss = 0

        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):
            with accelerator.accumulate(model):
                input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)

                output = model(input_ids=input_ids, attention_mask=attention_mask)

                loss = criterion(output, target.type_as(output))
                accelerator.backward(loss)
                optimizer.step()

                train_loss += loss.item()
                optimizer.zero_grad()

        print(f"Training loss is {train_loss/len(train_loader)}")
        val_loss = evaluate(model=model, criterion = criterion, dataloader=val_loader, device=device)
        print("Epoch {} complete! Validation Loss : {}".format(epoch, val_loss))


gc.collect()
torch.cuda.empty_cache()

LR = 1e-3
device = "cuda" if torch.cuda.is_available() else "cpu"
model = BertRegresser.from_pretrained(MODEL_NAME, config=config).to(device)
OPTIMIZER = optim.Adam(params=model.parameters(), lr=LR)
EPOCHS = 5
CRITERION = MCRMSELoss()
train(model=model,
      criterion=CRITERION,
      optimizer=OPTIMIZER,
      train_loader=train_loader,
      val_loader=valid_loader,
      epochs = EPOCHS,
     device = device)

evaluate(model, CRITERION, test_loader, 'cuda')